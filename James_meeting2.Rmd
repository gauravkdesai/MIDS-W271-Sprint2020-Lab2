---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 2'
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

## Instructions (Please Read Carefully):

* $\textbf{Due Date: Sunday March 15 2020, 11:59pm}$

* No page limit, but be reasonable

* Do not modify fontsize, margin or line-spacing settings

* One student from each group should submit the lab to their student github repository by the deadline; submission and revisions made after the deadline will not be graded

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab2.Rmd`
    * `StanCartman_KennyKyle_Lab2.pdf`
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* All answers should include a detailed narrative; make sure that your audience can easily follow the logic of your analysis. All steps used in modelling must be clearly shown and explained

* If you use libraries and functions for statistical modeling that we have not covered in this course, you must provide an explanation of why such libraries and functions are used and reference the library documentation

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity.

\newpage

# The Keeling Curve

In the 1950s, the geochemist Charles David Keeling observed a seasonal pattern in the amount of carbon dioxide present in air samples collected over the course of several years. He was able to attribute this pattern to the difference in the amount of land area and vegetation cover between the northern and southern hemipsheres, and the resulting variation in global rates of photosynthesis as the hemispheres' seasons alternated throughout the year. 

In 1958 Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present.

The `co2` data set in R's `datasets` package (automatically loaded with base R) is a monthly time series of atmospheric carbon dioxide concentrations measured in ppm (parts per million) at the Mauna Loa Observatory from 1959 to 1997. The curve graphed by this data is known as the 'Keeling Curve'.

```{r warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
# Start with a clean R environment
rm(list = ls())
# Set Fixed random seed to replicate the results
set.seed(28740)
# load libraries
library(Hmisc)
library(dplyr)
library(astsa)
library(Hmisc)
library(fable)
library(fpp3)
# plot time series for initial look
plot(co2, ylab = expression("CO2 ppm"), col = 'blue', las = 1)
title(main = "Monthly Mean CO2 Variation")
```

\newpage

**Part 1 (4 points)**

Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include thorough analyses of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed.

```{r}
# initial look at data
head(co2)
tail(co2)
str(co2)
summary(co2)
hist(co2)
```

We start by noting that there are no missing values, and that the series does not appear to be mean stationary, but rather, has a rising trend over time. Histogram of parts per million of co2 appears to be left-skewed rather than normal.
After an initial look at the data, we examine some aspects of the time series below.

```{r}
# sending the data to a tsibble object and examining head and tail
co2.ts <- as_tsibble(ts(co2, start=c(1959,1), frequency=12))
# plotting the series / ACF / PACF
co2.ts %>% gg_tsdisplay(plot_type="partial")
```

After converting the time series to a tsibble object, we see the concentration of parts per million steadily rising over time, confirming what we saw earlier. 
The ACF plot indicates a high degree of autocorrelation, as even after 24 lags, the series displays statistically significant autocorrelation. The PACF plot indicates that after lag 2, the 'memory' of the series starts to trail off, perhaps indicating an AR(2) model would be appropriate.  It should be noted however, that lags 12 and 13 also display statistical significance, perhaps indicating yearly seasonality, along with first differencing being appropriate.
To further investigate the trend and seasonality, we decompose the data into its component parts below.

```{r}
# decomposing the series into trend, seasonality, and remainder
co2.ts %>% model(STL(value)) %>% components() %>% autoplot()
```

This chart breaks the time series down into its component parts, with an upward trend clearly visible, as well as a regular seasonal trend, perhaps corroborating what was seen on the PACF chart. The seasonal trend also appears to be increasing in variance, which we will address below.
We now examine each of the component parts individually for higher clarity.

```{r}
# closer look at the trend component
co2.ts.components <- co2.ts %>% model(STL(value)) %>% components() 
ggplot(data=co2.ts.components)+
  geom_line(aes(x=index, y=trend)) +
  ggtitle("Trend")
```

Taking a closer look at the trend component of the data, we notice that there are no major shocks in the overall upward trajectory.

```{r}
# closer look at the seasonal component
co2.ts %>%
  gg_season(y=value, period = "year")+
  ylab("CO2 emission")+
    ggtitle("Seasonal plot : Monthly CO2 emission")
```

This chart takes a closer look at the seasonality of the data over time, clearly showing that parts per million is greatest every year around May and June, while it is lowest around September and October. Possible causes of this include weather patterns, vegetation, and energy usage. Hemispheric differences should also be taken into consideration, as the Northern and Southern hemisphers experience winter at opposite times of the year.

```{r}
# decomposition of ppm levels by month
co2.ts %>%
  gg_subseries(y=value, period = "year")+
  geom_hline(aes(yintercept=mean(co2.ts$value), colour="red"))+
  ylab("CO2 emission")+
  xlab("Years")+
  ggtitle("Seasonal subseries plot : Monthly CO2 emission")
```

Decomposing the parts per million levels by month, we see that the overall average of parts per million is at approximately 337. For a month like May, the average is at nearly 340, while in a month like October, the average is at approximately 334.
To address the increasing variance in seasonality that we noted earlier, we apply a Box-Cox transformation (below).

```{r}
lambda <- co2.ts %>%
  features(value, features=guerrero) %>%
  pull(lambda_guerrero)
co2.ts.trans.comp <- co2.ts %>% model(STL(box_cox(value, lambda))) 

co2.ts.trans.comp %>% components() %>% autoplot() +
  ggtitle(paste("Box-Cox Tranformed Decompositions for lambda=",round(lambda,3)))
```

Having examined the decomposition of the trend and seasonality, we turn our attention to the one remaining component; the residuals.

```{r}
# closer look at the residuals
co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  ggplot(aes(x=remainder)) + geom_histogram()
co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  autoplot()
co2.ts.trans.comp %>% components() %>% select("remainder") %>%
  gg_tsdisplay()
```

The first chart is a histogram that shows the residuals to be approximately normally distributed. The autocorrelation chart, however, shows evidence of seasonality, indicating 

**Part 2 (3 points)**

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a suitable polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#Convert to tsibble and create time index
co2.ts <- as_tsibble(co2.ts) %>% mutate(time_index = row_number())

#Create linear model
linear.trend.fit = lm(value ~ time_index, data=co2.ts)
summary(linear.trend.fit)

#Create dataframe of actual values, fitted values, and residuals
co2.df = data.frame(time=co2.ts$time_index, 
                    co2 = co2.ts$value,
                       fitted.values=linear.trend.fit$fitted.values,
                       residuals = linear.trend.fit$residuals)

#Plot actuals with fitted values
ggplot(data = co2.df, aes(x=time, y=value, colour=variable)) +
  xlab('Month Number Starting from 1959') +
  ylab('co2') +
  geom_line(aes(y=co2 , col='co2 actuals')) +
  geom_line(aes(y=fitted.values, col='Linear Trend Fit')) +
  ggtitle("co2 Actuals with Linear Trend Fit ") +
  ylim(200,700) +
  scale_y_continuous(labels = function(x) format(x/1000, scientific = FALSE)) + 
  theme(title = element_text(size = rel(1)),
        axis.text.y = element_text(angle = 45, hjust = 1)
        )	

#Create model diagnostic function
model_diagnostic = function(model) {
  plot(model)
  
  residualPlots(model)  
}

#Linear Model diagnostics
#model_diagnostic(linear.trend.fit)

```

```{r}
require(forecast)
pred = predict(linear.trend.fit, n.ahead = 36)
#plot(data,type='l',xlim=c(2004,2018),ylim=c(1,1600),xlab = 'Year',ylab = 'ppm')
```

```{r}
new <- data.frame(time_index = seq(469, 500, 1))
predict(linear.trend.fit, new, se.fit = F)
```



```{r}
library(fpp2)
# forecast needs ts object
forecast(co2, h=2)
```

```{r}
library(lmtest)
futurVal <- forecast(linear.trend.fit, h=10, level=c(99.5))
plot.forecast(futurVal)
```


```{r}
# putting the time series into a data frame to make things easier
co2.df <- data.frame(index = seq(1:468), date = co2.series$index, value = co2.series$value)
head(co2.df)
```


```{r}
# constructing a linear time trend and capturing the residuals
linear.co2 <- lm(value ~ index, data=co2.df) 
summary(linear.co2)
co2.df$linear.resid <- linear.co2$residuals

# plotting the residuals of the linear time trend
ggplot(data=co2.df, aes(x = co2.df$index, y = co2.df$linear.resid)) + geom_point()
```

The expected value of the residuals does not appear to be zero, and appears to vary over time, meaning a linear trend is not well suited to this data.

```{r}
# constructing a quadratic time trend and capturing the residuals
quadratic.co2 <- lm(value ~ index + I(index^2), data=co2.df)
summary(quadratic.co2)
co2.df$quad.resid <- quadratic.co2$residuals

# plotting the residuals of the quadratic time trend
ggplot(data=co2.df, aes(x = co2.df$index, y = co2.df$quad.resid)) + geom_point()
```

The expectation of the residuals appears to be much closer to zero, indicating that a higher order polynomial is a better fit than a linear time trend. There does still appear to be some variation in the residuals with time, and so it may also be possible that even a higher order polynomial could be more appropriate. In addition, a logarithmic transformation could be appropriate if it were thought that the variance of the time series was increasing with time. From the original time series plot, however, this does not appear to be the case, and we decide not to utilize one in our final specification,

```{r}
# constructing a cubic time trend and capturing the residuals
cubic.co2 <- lm(value ~ index + I(index^2) + I(index^3), data=co2.df)
summary(cubic.co2)
co2.df$cubic.resid <- cubic.co2$residuals

# plotting the residuals of the cubic time trend
ggplot(data=co2.df, aes(x = co2.df$index, y = cubic.co2$residuals)) + geom_point()
```

Judging from the residuals, this cubic trend does the best job or removing any relationship between the residuals and time.

```{r}
# constructing a cubic time trend with seasonal coefficients and capturing the residuals
Seas <- cycle(co2)
seasonal.co2 <- lm(co2.df$value ~ co2.df$index + I(co2.df$index^2) + I(co2.df$index^3) + factor(Seas))
summary(seasonal.co2)
co2.df$seasonal.resid <- seasonal.co2$residuals

# plotting the residuals of the seasonal time trend
ggplot(data=co2.df, aes(x = co2.df$index, y = seasonal.co2$residuals)) + geom_point()
```

The inclusion of a dummy seasonality variable does not appear to do a good job of specifying the data, as there now appears to be a pattern in the residuals that was not there before.

```{r}
library(forecast)
predict.data = data.frame(x = seq(469, 756, 1))
predict(linear.co2, newdata=data.frame(x = seq(469,756,1)), se.fit=TRUE)
```

**Part 3 (3 points)**

Following all appropriate steps, choose an ARIMA model to fit to the series. Discuss the characteristics of your model and how you selected between alternative ARIMA specifications. Write your model (or models) using backshift notation. Use your model (or models) to generate forecasts to the year 2020. 

Step 1: Conduct EDA to determine if a transformation is necessary to make stationary
- This step has already been done and a transformation will be necessary

Step 2: Transform the series if needed
- This step is carried out below through first differencing

```{r}
# creating a dataframe of differenced values and plotting
diff.df <- data.frame(index = co2.df$index[-1], difference = diff(co2))
ggplot(diff.df, aes(x=index, y=difference)) + geom_line()
```

Step 3: Estimate several `ARIMA(p,d,q)x(P,D,Q)s` models, with starting values coming from the examination of time series plot, ACF, and PACF.
- The PACF plot indicates yearly seasonality, so we begin with a model of ARIMA(0,0,0)(1,1,0) as a baseline 

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
get.best.arima <- function(x.ts, maxord = c(1,1,1,1,1,1))
{
  best.aic <-1000000
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
      for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in 0:maxord[6])
      {
        fit <- Arima(x.ts, order = c(p,d,q),
                     seas = list(order = c(P,D,Q),
                                 frequency(x.ts)), method = "CSS")
        fit.aic <- -2*fit$loglik + (log(n) + 1) * length(fit$coef)
        if (fit.aic < best.aic)
        {
          best.aic <- fit.aic
          best.fit <- fit
          best.model <- c(p,d,q,P,D,Q)
        }
      }
    list(best.aic, best.fit, best.model)
}

best.arima <- get.best.arima(co2, maxord = c(2,2,2,2,2,2))
best.arima

library("forecast")
#futurVal <- forecast(best.arima,h=10, level=c(99.5))

#co2 %>%
#  Arima(order=c(1,1,1), seasonal=c(2,0,2), method = "CSS", lambda=0) %>%
#  forecast(h = 264) %>%
#  autoplot() +
#    ylab("co2") + xlab("Year") + ggtitle("co2 Forecasts for ARIMA(0,1,1)(1,1,2)[12]")
```


**Part 4 (4 points)**

The file `co2_weekly_mlo.txt` contains weekly observations of atmospheric carbon dioxide concentrations measured at the Mauna Loa Observatory from 1974 to 2020, published by the National Oceanic and Atmospheric Administration (NOAA). Convert these data into a suitable time series object, conduct a thorough EDA on the data, and address the problem of missing observations. Describe how the Keeling Curve evolved from 1997 to the present and compare current atmospheric CO2 levels to those predicted by your forecasts in Parts 2 and 3. Use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2 and 3 over the entire period.

```{r}
setwd('/Users/jamesdarmody/Documents/GroupLab2w217/MIDS-W271-Sprint2020-Lab2')
co2_mlo <- read.table('co2_weekly_mlo.txt', header=F)
colnames(co2_mlo) <- c('yr', 'mon', 'day', 'decimal', 'ppm', '#days', '1yr ago', '10yr ago', 'since 1800')
head(co2_mlo)
```

```{r}
# taking a look at weekly ppm
head(co2_mlo[,5])
str(co2_mlo[,5])
summary(co2_mlo[,5])
missing_ppm <- length(which(co2_mlo[,5] == -999.99))
print(cat('Missing Values', missing_ppm))
```

We notice that there are approximately 20 missing values in the time series for weekly ppm. We propose using the cubic spline imputation method from the zoo library below

```{r}
# assigning missing values to nothing so they can be interpolated
co2_mlo[,5][co2_mlo[,5] == -999.99] <- ""
#df$depth[df$depth<10] <- 0
library(zoo)
co2_mlo_ts <- ts(co2_mlo$ppm, frequency=52, start=c(1974,5))
co2_mlo_ts_full <- na.spline(co2_mlo_ts)
```

We now examine the series again for missing values and find none

```{r}
# checking again for missing values
missing_ppm2 <- length(which(co2_mlo_ts_full == -999.99))
missing_ppm2
```

```{r}
# sending the data to a tsibble object and examining head and tail
co2_mlo_tsibble <- as_tsibble(ts(co2_mlo_ts_full, freq=52, start=c(1974,5)))
# plotting the series / ACF / PACF
plot(co2_mlo_tsibble)
```

```{r}
# decomposing the series into trend, seasonality, and remainder
co2_mlo_tsibble %>% model(STL(value)) %>% components() %>% autoplot()
```


**Part 5 (3 points)**

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

**Part 6 (3 points)**

Generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric C02 levels in the year 2100. How confident are you that these are accurate predictions?










