---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 2'
subtitle: James Darmody, Maria Vasiliadis and Gaurav Desai
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

```{r warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

# Start with a clean R environment
rm(list = ls())

# Set Fixed random seed to replicate the results
set.seed(28740)

library(Hmisc)
library(gridExtra)
library(dplyr)
library(purrr)
library(imputeTS)
library(tsibble)
library(fabletools)
library(feasts)
library(fable)
library(lubridate)
library(forecast)
library(seasonal)
library(tsbox)
library(tibble)
library(svMisc)
```

# The Keeling Curve

In the 1950s, the geochemist Charles David Keeling observed a seasonal pattern in the amount of carbon dioxide present in air samples collected over the course of several years. He was able to attribute this pattern to the difference in the amount of land area and vegetation cover between the northern and southern hemipsheres, and the resulting variation in global rates of photosynthesis as the hemispheres' seasons alternated throughout the year. 

In 1958 Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present.

The `co2` data set in R's `datasets` package (automatically loaded with base R) is a monthly time series of atmospheric carbon dioxide concentrations measured in ppm (parts per million) at the Mauna Loa Observatory from 1959 to 1997. The curve graphed by this data is known as the 'Keeling Curve'.

```{r}
plot(co2, ylab = expression("CO2 ppm"), col = 'blue', las = 1)
title(main = "Monthly Mean CO2 Variation")
```

\newpage

**Part 1 (4 points)**

Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include thorough analyses of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed.

**Answer**
Lets look at the data
```{r}
head(co2)
tail(co2)
str(co2)
summary(co2)
ggplot(co2, aes(x = co2)) +
  geom_histogram(color = "darkblue", fill = "lightblue") +
  labs(title = "Histogram co2 ppm")
```
The co2 dataset is comprised of 468 observations of data from 1959 to 1998 where each datapoint is one month's data. The co2 levels in the dataset range from between 313.2 to 366.8. The histogram shows us that the most common co2 levels are between 320 and 325, and generally the frequency of co2 values decreases as co2 goes up. There is a small spike of frequencies between co2 level 355 and 360. There are no missing values in the dataset. Histogram of value is left skewed but that may not be correct representation of the co2 emission levels over time. So lets look at it using time dimension.

```{r}
co2.ts <- as_tsibble(co2)
autoplot(co2.ts, .vars = value)
```
We observe two things
1. There is upward trend year on year in CO2 emissions
2. There is a seasonal fluctuation within a year

Lets try to decompose and see the individual components in more detail

```{r}
co2.ts %>% model(STL(value)) %>% components() %>% autoplot()
```
When we decompose using STL model, we can see the upward trend separated from yearly seasonal cycles.

We note that seasonal cycle ups and downs are growing with time, so we may need to stabilize the seasons for better predictions.


```{r}
co2.ts %>% gg_tsdisplay(plot_type = "partial", y = value) +
  ggtitle("Trend with ACF & PACF")
```

The ACF tells us that there is autcorrelation even after 24 months. On the other hand PACF drops after 2 but again pops up at multples of 12 months. This indicates that the model could be AR model with 12 months seasonal cycles.

Now lets look at each component one by one starting with trend:

```{r}
co2.ts.components <- co2.ts %>% model(STL(value)) %>% components()
ggplot(data = co2.ts.components) +
  geom_line(aes(x = index, y = trend)) +
  ggtitle("Trend of co2 Time Series - ppm")
```
There are minor bumps and dips but there is no major shock or sudden movement in trend.

Now lets look at seasonal cycles.

```{r}
co2.ts %>%
  gg_season(y = value, period = "year") +
  ylab("CO2 emission") +
  ggtitle("Seasonal plot : Monthly CO2 emission")
```
We can clearly see that CO2 emission is higher in April-May and goes down in September-October every year. This could be due to summer in Northern hemisphere vs southern hemisphere. Population, vegitation and other attributes of these two halves of earth are quite different which could potentially cause this seasonal effect.

Now lets look at same data from another angle.
```{r}
co2.ts %>%
  gg_subseries(y = value, period = "year") +
  geom_hline(aes(yintercept = mean(co2.ts$value)), colour = "red") +
  ylab("CO2 emission") +
  xlab("Years") +
  ggtitle("Seasonal subseries plot : Monthly CO2 emission")
```
It is more or less same observation that northern summer increases CO2 emission and southern summer decreases CO2 eission within a year. But we can cearly see that CO2 emission is on the rise year on year for each and every month.

Since we saw earlier that seasonal patterns are are increasin in variance, we can try to transform it so the seasonal changes are uniform across time series. This is desirable property for finding a good model fit.

```{r}
lambda <- co2.ts %>%
  features(value, features = guerrero) %>%
  pull(lambda_guerrero)

co2.ts.trans <-
  as_tsibble(ts(box_cox(co2.ts$value, lambda), start = c(1959, 1)))
co2.ts.trans.comp <- co2.ts %>% model(STL(box_cox(value, lambda)))

co2.ts.trans.comp %>% components() %>% autoplot() +
  ggtitle(paste(
    "Box-Cox Tranformed Decompositions for lambda=",
    round(lambda, 3)
  ))
```
After transformation we can see the seasonal pattern variance remains constant throughout.


Now lets look at residuals:

```{r}
#Before box-cox transformation
plot.co2.ts.components <-
  co2.ts.components  %>% select("remainder") %>%
  ggplot(aes(x = remainder)) + geom_histogram() +
  ggtitle("Residuals Prior to Box-Cox")

#After box-cox transformation
plot.co2.ts.trans.comp <-
  co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  ggplot(aes(x = remainder)) + geom_histogram() +
  ggtitle("Residuals Subsequent to Box-Cox")

grid.arrange(plot.co2.ts.components, plot.co2.ts.trans.comp, nrow = 2)
```
Both before and after box-cox transformation residuals look fairly normally distributed.

```{r}

plot.resid.co2.ts.components <-
  co2.ts.components %>% select("remainder") %>%
  autoplot() + ggtitle("Residuals Prior to Box-Cox Transformation")

plot.resid.co2.ts.trans.comp <-
  co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  autoplot() + ggtitle("Residuals Subsequent to Box-Cox Transformation")

grid.arrange(plot.resid.co2.ts.components,
             plot.resid.co2.ts.trans.comp,
             nrow = 2)
```

Residuals appear to be stationary and appear to be white noise with no trend or seasonal patterns. 

```{r}
co2.ts.components %>% select("remainder") %>%
  gg_tsdisplay(plot_type = "partial") + ggtitle("Residuals with ACF Prior to Box-Cox")

co2.ts.trans.comp %>% components() %>% select("remainder") %>%
  gg_tsdisplay(plot_type = "partial") + ggtitle("Residuals with ACF Post Box-Cox")

```

**Part 2 (3 points)**

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a suitable polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. 

Lets first fit a linear time trend model:

```{r}
time.index <- 1:length(co2.ts$index)
mod.ln.1 <- lm(formula = value ~ time.index, data = co2.ts)
par(mfrow = c(2, 2))
summary(mod.ln.1)
plot(mod.ln.1)
```

The residuals show a pattern indicating linear model is not able to capture the curvature of the trend. Lets try quadratic term:

```{r}
mod.ln.2 <-
  lm(formula = value ~ time.index + I(time.index ^ 2),
     data = co2.ts)
summary(mod.ln.2)
par(mfrow = c(2, 2))
plot(mod.ln.2)
```

The residuals are showing a much better fit. Now lets try cubic term and see if we get any more improvements.

```{r}
mod.ln.3 <-
  lm(formula = value ~ time.index + I(time.index ^ 2) + I(time.index ^ 3),
     data = co2.ts)
summary(mod.ln.3)
par(mfrow = c(2, 2))
plot(mod.ln.3)
```
This model is even better fit, but Q-Q plot shows deviation at start and end. So lets try taking log of value and see if that scale fit any better.

```{r}
mod.ln.4 <-
  lm(
    formula = log(value) ~ time.index + I(time.index ^ 2) + I(time.index ^ 3),
    data = co2.ts
  )
summary(mod.ln.4)
par(mfrow = c(2, 2))
plot(mod.ln.4)
```

Taking log doesn't improve much.

So finally lets choose the best of the models, which so far is cubic model, and add seasonal dummy variables for Jan to December:

```{r}
co2.df <-
  data.frame(
    value = co2.ts$value,
    time.index = time.index,
    season = factor(time.index %% 12, ordered = F)
  )
mod.ln.5 <-
  lm(
    formula = value ~ 0 + time.index + I(time.index ^ 2)  + I(time.index ^ 3) + season,
    data = co2.df
  )
par(mfrow = c(2, 2))
summary(mod.ln.5)
plot(mod.ln.5)
```

This model has better R-square than any of the previous models. Lets use this model to forecast for year 2020:

```{r}
time.index.2020 = seq(from = (2020 - 1959) * 12 + 1, length.out = 12)
co2.2020.df <-
  data.frame(time.index = time.index.2020,
             season = factor(time.index.2020 %% 12, ordered = F))

co2.2020.pred <- predict(object = mod.ln.5, newdata = co2.2020.df)

co2.2020.pred.ts <-
  as_tsibble(ts(
    data = co2.2020.pred,
    start = c(2020, 1),
    frequency = 12
  ))

co2.2020.pred.ts %>%
  autoplot(.vars = value)
```
Unsurprisingly, the 2020 prediction follows the same seasonal trend as the other years, with a max co2 level of ~386.5 and a minimum value of ~380.5. 
**Part 3 (3 points)**

Following all appropriate steps, choose an ARIMA model to fit to the series. Discuss the characteristics of your model and how you selected between alternative ARIMA specifications. Write your model (or models) using backshift notation. Use your model (or models) to generate forecasts to the year 2020.

To fit an ARIMA model, we write a function that will evaluate different ARIMA combinations of p, d, q, P, D, and Q by calculating aic, aicc, and bic values for each combination. Because of the large number of models that can be fit, we will have maximum values of 2 for p, q, P, and Q, and a maximum value of 1 for d and D.

```{r}
no.p = no.P = 2
no.d = no.D = 1
no.q = no.Q = 2
no.of.models <- (no.p + 1) * (no.d + 1) * (no.q + 1) * (no.P + 1) * (no.D +
                                                                       1) * (no.Q + 1)

print(paste("Number of models to fit =", no.of.models))

params.df <-
  expand.grid(
    p = 0:no.p,
    d = 0:no.d,
    q = 0:no.q,
    P = 0:no.P,
    D = 0:no.D,
    Q = 0:no.Q
  )
i <- 1

funFitModel <- function(param.row) {
  progress(
    value = i,
    max.value = no.of.models,
    console = TRUE,
    progress.bar = TRUE
  )
  i <- i + 1
  p = param.row['p']
  d = param.row['d']
  q = param.row['q']
  P = param.row['P']
  D = param.row['D']
  Q = param.row['Q']
  
  #print(paste(p,q,d,P,Q,D))
  tryCatch({
    model.fit = Arima(
      y = as.ts(co2.ts),
      order = c(p, d, q),
      seasonal = c(P, D, Q),
      lambda = lambda,
      include.drift = FALSE
    )
    
    model.info = data.frame(p, d, q, P, D, Q, model.fit$aic, model.fit$aicc, model.fit$bic)
    return (model.info)
    
  }, error = function(e) {
    return (data.frame()) # Empty DF
  })
  
}

model.fit.info.df <-
  do.call("rbind", (apply(params.df, 1, funFitModel)))
colnames(model.fit.info.df) <-
  c("p", "d", "q", "P", "D", "Q", "aic", "aicc", "bic")
print("Top 6 models by BIC")
model.fit.info.df %>%
  arrange(bic) %>%
  head()
```

Our function evaluated 324 models. Using BIC as criteria we can see that ARIMA (0,1,1) with seasonality (2,0,1) as best model for CO2 time series. BIC for this model is least -5389.765. Note that this model uses lambda= -0.03432107 and no drift.

Lets save the model to forecast for year 2020:

```{r}
model.arima.best.q3 = Arima(
  y = as.ts(co2.ts),
  order = c(0, 1, 1),
  seasonal = c(2, 0, 1),
  lambda = lambda,
  include.drift = FALSE
)

co2.2020.arima.pred.ts <- forecast(model.arima.best.q3, h = 276) %>%
  as_tibble() %>%
  dplyr::select('Point Forecast') %>%
  ts(start = c(1998, 1), frequency = 12) %>%
  as_tsibble() %>%
  filter_index("2019-12-31" ~ .)

co2.2020.arima.pred.ts %>%
  autoplot(.vars = value) +
  labs(title = "2020 CO2 predictions from best fitted ARIMA (0,1,1) (2,0,1) model")
```
Our 2020 prediction using the ARIMA model follows the seasonal pattern previously recognized, and has a maximum CO2 level of 404.5 and a minimum level of 397.5

Below, we inscribe the model using Latex and Backshift notation.

$$
  x_{t}(1-B) - \alpha B^{24}x_{t}(1-B) = Bw_{t}
$$


**Part 4 (4 points)**

The file `co2_weekly_mlo.txt` contains weekly observations of atmospheric carbon dioxide concentrations measured at the Mauna Loa Observatory from 1974 to 2020, published by the National Oceanic and Atmospheric Administration (NOAA). Convert these data into a suitable time series object, conduct a thorough EDA on the data, and address the problem of missing observations. Describe how the Keeling Curve evolved from 1997 to the present and compare current atmospheric CO2 levels to those predicted by your forecasts in Parts 2 and 3. Use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2 and 3 over the entire period.  

First, we read in and examine the dataset:

```{r}
co2.noaa.weekly.df <-
  read.table(
    'co2_weekly_mlo.txt',
    header = FALSE,
    comment.char = '#',
    na.strings = '-999.99'
  )
colnames(co2.noaa.weekly.df) <-
  c(
    'year',
    'month',
    'day',
    'decimal.day',
    'record.value',
    '#days',
    'one.year.ago',
    'ten.years.ago',
    'since.1800.diff'
  )
```

```{r}
head(co2.noaa.weekly.df)
tail(co2.noaa.weekly.df)
```

```{r}
summary(co2.noaa.weekly.df) # 20 readings are missing
describe(co2.noaa.weekly.df)
co2.noaa.weekly.df <- co2.noaa.weekly.df %>%
  mutate(record.date = ymd(paste(year, month, day)))
```
We can see in the dataset there are 2388 observations. The make the time series easier to work with, we create an index using all of the time columns and convert the data to a tsibble object. 

```{r}
week.frequency = 365.25 / 7
co2.noaa.weekly.ts.raw <-
  ts(data = co2.noaa.weekly.df$record.value,
     start = 1974.3795	,
     frequency = week.frequency)
co2.noaa.weekly.ts <-
  as_tsibble(
    ts(
      data = co2.noaa.weekly.df$record.value,
      start = 1974.3795	,
      frequency = week.frequency
    ),
    class = "matrix"
  )
cbind(head(co2.noaa.weekly.ts), tail(co2.noaa.weekly.ts))
summary(co2.noaa.weekly.ts)
class(co2.noaa.weekly.ts)
```
In the summary of the tsibble object, we can see that there are 20 missing values. In the available data, the lowest CO2 value is 326.7 and the greatest is 415.4. 

To fill in the missing data, we fill in the values with interpolation from neighbour data points. After the interpolation we can see that there are 0 rows with missing values. 
```{r}
co2.noaa.weekly.ts %>%
  filter(is.na(value))

co2.noaa.weekly.ts <-
  na_interpolation(co2.noaa.weekly.ts, option = "spline")

co2.noaa.weekly.ts %>%
  filter(is.na(value))
```
Now that we have a complete dataset, we can plot the Keeling curve and continue with EDA.
```{r}
co2.noaa.weekly.ts %>%
  autoplot(.vars = value) +
  geom_smooth() +
  labs(title = "Keeling Curve till 2020", y = "Co2 emission (units)", x =
         "Year")
```
In the Keeling curve, we saw a constant increasing trend from the start of the series to about 1990. After a few years of a slower rate of increase, we can see that from about 1995 onwards the trend has picked up and increasing faster than pre 1995 trend. In the curve we see the clear seasonal pattern in CO2 levels as every year the CO2 levels experience a spike and low point.

```{r}
#Additional EDA
co2.noaa.weekly.ts %>%
  gg_season(y = value, period = "year") +
  ylab("CO2 emission") +
  ggtitle("Seasonal plot : Monthly CO2 emission")
```
The seasonal plot looks similar to the monthly CO2 data evaluated earlier, but we can now see the effects of the weekly data points by the variation in each line versus the smoother line in the monthly data. The seasonal monthly trends, however, are clearly the same, with a high in May and low in October. 

```{r}
co2.noaa.weekly.ts %>% gg_tsdisplay(plot_type = "partial", y = value) +
  ggtitle("Trend with ACF & PACF")
```
The ACF tells us that there is autcorrelation even after 24 months. On the other hand PACF is not indicative of any partial autocorrelation. This could have something to do with the fact that the series is a weekly series. 

Now lets compare the current CO2 levels to the predicted levels from Q2 and Q3. In the chart below, the blue line represents CO2 actuals, red represents forecasts from the cubic model with monthly dummy variables, and navy represents the forecast from the ARIMA model:

```{r}
co2.noaa.weekly.ts %>%
  filter_index("2019-12-31" ~ .) %>%
  mutate(index = as.Date(index)) %>%
  autoplot(.vars = value, color = "blue") +
  autolayer(co2.2020.pred.ts, .vars = value, color = "red") +
  autolayer(co2.2020.arima.pred.ts, .vars = value, color = "navy") +
  labs(title = "Year 2020 Actual vs Prediction from Linear Model and ARIMA model") 
```
For the first few actual observations in 2020, we see that the actual CO2 levels (blue) are higher than the values predicted from both models. The actuals are around 10-15 units higher than the ARIMA model forecast (navy). The ARIMA model and the cubic plus dummy model follow the same general pattern, but the ARIMA model has forecasted values around 15-20 units greater than the cubic model (red). 

Now lets compare monthly average curve with model forecasts. In the chart below, the blue line represents CO2 actuals using monthly averages, red represents forecasts from the cubic model with monthly dummy variables, and navy represents the forecast from the ARIMA model:
```{r}
#First lets convert to monthly time series
co2.noaa.monthly.ts <- co2.noaa.weekly.ts %>%
  as_tibble() %>%
  mutate(yearmonth = yearmonth(yearweek(index))) %>%
  group_by(yearmonth) %>%
  summarise(avg_value = mean(value)) %>%
  as_tsibble(index = yearmonth, value = avg_value)

summary(co2.noaa.monthly.ts) #> 1974 May to 2020 Feb

# Now predict using linear model for same time frame
time.index.1974.may = seq(from = (1974 - 1959) * 12 + 5, length.out = 550)
co2.noaa.like.df <-
  data.frame(time.index = time.index.1974.may,
             season = factor(time.index.1974.may %% 12, ordered = F))
co2.noaa.like.pred <-
  predict(object = mod.ln.5, newdata = co2.noaa.like.df)
co2.noaa.like.pred.ts <-
  as_tsibble(ts(
    data = co2.noaa.like.pred,
    start = c(1974, 5),
    frequency = 12
  ))

# now predict using Arima model selected in Q3
co2.noaa.arima.pred.ts <- forecast(model.arima.best.q3, h = 276) %>%
  as_tibble() %>%
  dplyr::select('Point Forecast') %>%
  ts(start = c(1998, 1), frequency = 12) %>%
  as_tsibble()

model.arima.best.q3.fitted.ts <- model.arima.best.q3$fitted %>%
  as_tsibble()

co2.noaa.monthly.ts %>%
  autoplot(.vars = avg_value, color = "blue") +
  autolayer(co2.noaa.like.pred.ts, .vars = value, color = "red") +
  autolayer(co2.noaa.arima.pred.ts, .vars = value, color = "navy") +
  autolayer(model.arima.best.q3.fitted.ts,
            .vars = value,
            color = "navy") +
  labs(title = "Part 2 and 3 model predictions vs NOAA monthly average")
```
We see that up until about the year 2000, both models follow the actual data closely. After, the models separate from the actuals with the cubic model showing the lowest values (red), and the ARIMA model (navy) also showing lower values than the actual values (blue). The actual CO2 levels are rising faster than either model forecasted. The cubic model forecasts that the CO2 level would stop rising and would flatten. Clearly the ARIMA model is closer to modeling the actual CO2 values and the cubic model is not a good fit. 

**Part 5 (3 points)**

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.


First lets try couple of ways in which we can decompose seasonality by analysing residuals:

```{r}
plotRemainderFromSTL <- function(s.degree = 0) {
  stl.val <- co2.noaa.weekly.ts %>%
    stl(s.window = week.frequency, s.degree = s.degree)
  ts.obj <-
    as_tsibble(ts(
      stl.val$time.series[, 'remainder'],
      start = 1974.3795	,
      frequency = week.frequency
    ))
  
  par(mfrow = c(1, 2))
  ts.obj %>% gg_tsdisplay(y = value, plot_type = "partial") #TODO add s.degree in title
}

plotRemainderFromSTL(s.degree = 0)
plotRemainderFromSTL(s.degree = 1)
```
We can see that with degree of locality 1 in seasonal extraction, we get much better residuals with dampening ACF and sharp decrease in PACF at lag 1. There is still some seasonality left, but it is better than degree 0.

Lets see how seasonal decomposition looks:
```{r}
co2.noaa.weekly.ts %>%
  stl(s.window = week.frequency, s.degree = 1) %>%
  autoplot()
```
In the decomposed series we see the clear increasing trend line and the seasonal pattern with somewhat consistent variance. 

Now lets substract seasonal data from time series to get seasonally adjusted time series:

```{r}
co2.noaa.weekly.ts.components <-
  stl(co2.noaa.weekly.ts , s.window = week.frequency)$time.series
co2.noaa.weekly.ts.components.seasonal <-
  co2.noaa.weekly.ts.components[, 'seasonal']
co2.noaa.weekly.ts.SA <-
  as_tsibble(
    ts(
      co2.noaa.weekly.ts$value - co2.noaa.weekly.ts.components.seasonal,
      start = 1974.3795	,
      frequency = week.frequency
    )
  )
cbind(
  head(co2.noaa.weekly.ts),
  tail(co2.noaa.weekly.ts),
  head(co2.noaa.weekly.ts.SA),
  tail(co2.noaa.weekly.ts.SA)
)
```


Lets check the SA time series
```{r}
co2.noaa.weekly.ts.SA %>%
  autoplot()

co2.noaa.weekly.ts.SA %>%  gg_tsdisplay(plot_type = "partial")
```
We can see a smoother trend line with the seasonally adjusted series. The acf chart shows a very slow decreasing lag for every 30+ lags. The pacf chart suggests a non-stationary random walk process. 

Now we split the seasonally adjusted and non-seasonally adjusted time series into training and test sets by taking the last 2 years of data as the test set.

```{r}
trainTestSplit <- function(time.series) {
  time.series.test <- time.series %>%
    filter_index("2018-01-01" ~ .)
  
  time.series.train <- time.series %>%
    filter_index(. ~ "2017-12-31")
  
  return (list(train = time.series.train, test = time.series.test))
}

#Split SA data into training and test

co2.noaa.weekly.ts.SA.train.test <-
  trainTestSplit(co2.noaa.weekly.ts.SA)
co2.noaa.weekly.ts.SA.test <- co2.noaa.weekly.ts.SA.train.test$test
co2.noaa.weekly.ts.SA.training <-
  co2.noaa.weekly.ts.SA.train.test$train

#Split NSA data into training and test
co2.noaa.weekly.ts.train.test <- trainTestSplit(co2.noaa.weekly.ts)
co2.noaa.weekly.ts.test <- co2.noaa.weekly.ts.train.test$test
co2.noaa.weekly.ts.training <- co2.noaa.weekly.ts.train.test$train
```

Now lets fit Arima model for SA and non SA time series using a function that evaluates different combinations of p, d, q, P, D, and Q. For the seasonally adjusted model, we set P, D, and Q to 0, and we can test greater values of p, d, and q because there are less combinations to attempt given there is no seasonal component. Therefore, we evaluate different combinations of models using a max value of 3 for p and q and a max value of 2 for d. 

```{r}
#Fit ARIMA model for SA data
no.p = 3
no.d = 2
no.q = 3

# P,D, Q are set to 0 because training data is already seasonally adjusted
no.P = 0
no.D = 0
no.Q = 0

no.of.models <- (no.p + 1) * (no.d + 1) * (no.q + 1) * (no.P + 1) * (no.D +
                                                                       1) * (no.Q + 1)

print(paste("Number of models to fit =", no.of.models))

params.df <-
  expand.grid(
    p = 0:no.p,
    d = 0:no.d,
    q = 0:no.q,
    P = 0:no.P,
    D = 0:no.D,
    Q = 0:no.Q
  )
i <- 1

funFitModel <- function(param.row) {
  progress(
    value = i,
    max.value = no.of.models,
    console = TRUE,
    progress.bar = TRUE
  )
  i <- i + 1
  p = param.row['p']
  d = param.row['d']
  q = param.row['q']
  P = param.row['P']
  D = param.row['D']
  Q = param.row['Q']
  
  #print(paste(p,d,q,P,D,Q))
  tryCatch({
    model.fit = Arima(
      y = as.ts(co2.noaa.weekly.ts.SA.training),
      order = c(p, d, q),
      seasonal = c(0, 0, 0),
      include.drift = FALSE
    )
    
    model.info = data.frame(p, d, q, P, D, Q, model.fit$aic, model.fit$aicc, model.fit$bic)
    
    
    return (model.info)
    
  }, error = function(e) {
    return (data.frame())
  })
  
}

model.fit.info.df.SA <-
  do.call("rbind", (apply(params.df, 1, funFitModel)))
colnames(model.fit.info.df.SA) <-
  c("p", "d", "q", "P", "D", "Q", "aic", "aicc", "bic")
```
```{
r
}
print("Top 6 models by BIC")
model.fit.info.df.SA %>%
arrange(bic) %>%
head()
```

Arima(0,2,3) is in top 3 models by all 3 measres AIC, AICc and BIC. We pick this model as best model as this has least number of parameters compared to other top models.

```{r}
model.arima.best.SA = Arima(
  y = as.ts(co2.noaa.weekly.ts.SA.training),
  order = c(0, 2, 3),
  seasonal = c(0, 0, 0),
  include.drift = FALSE
)

summary(model.arima.best.SA)
checkresiduals(model.arima.best.SA)
```
We can conclude from the residuals chart that the model is an ok fit. The mean appears to be 0, but there may be some uncaptured seasonality as shown in the acf chart. 

Now lets fit Arima model to non-SA time series. Because of the number of models to fit and the time taken to evaluate all models, we set a max value of 1 for all parameters p, d, q, P, D, and Q. 

```{r}
#Fit ARIMA model for NSA data
no.p = no.P = 1
no.q = no.Q = 1
no.d = no.D = 1
no.of.models <- (no.p + 1) * (no.d + 1) * (no.q + 1) * (no.P + 1) * (no.D +
                                                                       1) * (no.Q + 1)

print(paste("Number of models to fit =", no.of.models))

params.df <-
  expand.grid(
    p = 0:no.p,
    d = 0:no.d,
    q = 0:no.q,
    P = 0:no.P,
    D = 0:no.D,
    Q = 0:no.Q
  )
i <- 1

funFitModel <- function(param.row) {
  progress(
    value = i,
    max.value = no.of.models,
    console = TRUE,
    progress.bar = TRUE
  )
  i <- i + 1
  p = param.row['p']
  d = param.row['d']
  q = param.row['q']
  P = param.row['P']
  D = param.row['D']
  Q = param.row['Q']
  
  #print(paste(p,q,d,P,Q,D))
  tryCatch({
    model.fit = Arima(
      y = as.ts(co2.noaa.weekly.ts.training),
      order = c(p, d, q),
      seasonal = c(P, D, Q),
      include.drift = FALSE
    )
    
    model.info = data.frame(p, d, q, P, D, Q, model.fit$aic, model.fit$aicc, model.fit$bic)
    
    
    return (model.info)
    
  }, error = function(e) {
    return (data.frame())
  })
  
}

model.fit.info.df.NSA <-
  do.call("rbind", (apply(params.df, 1, funFitModel)))
colnames(model.fit.info.df.NSA) <-
  c("p", "d", "q", "P", "D", "Q", "aic", "aicc", "bic")
```
```{
r
}
print("Top 6 models by BIC")
model.fit.info.df.NSA %>%
arrange(bic) %>%
head()
```

Best model for NSA time sries is Arima(1,1,1)(0,1,1). We selected this model because has lowest AIC, AICc and BIC so this is natural selection.
Ideally, we could try with higher values of p,d and q if but it is going to take much longer to estimate the models as number of parameters quickly increase exponentially.

```{r}
model.arima.best.NSA = Arima(
  y = as.ts(co2.noaa.weekly.ts.training),
  order = c(1, 1, 1),
  seasonal = c(0, 1, 1),
  include.drift = FALSE
)

summary(model.arima.best.NSA)
checkresiduals(model.arima.best.NSA)
```
We can conclude from the residuals chart that the model is an ok fit. The mean appears to be 0, but there may be some uncaptured seasonality as shown in the acf chart. 

Now lets compare how these selected model perform in-sample and pseudo out-of-sample data: 

```{r}
# in-sample
in.sample.accuracy = data.frame(rbind(
  accuracy(model.arima.best.SA),
  accuracy(model.arima.best.NSA)
), row.names = c("SA", "NSA"))
in.sample.accuracy
```
If we look at the in-sample accuracy of our SA and NSA best models, the SA model marginally does better than NSA, but they are very close. 

Lets look at actual vs fitted values in chart.

```{r}
co2.noaa.weekly.ts.SA.training %>%
  autoplot(.vars = value, color = "red") +
  autolayer(as_tsibble(model.arima.best.SA$fitted),
            .vars = value,
            color = "blue") +
  labs(title = "SA In-Sample - Actual vs Fitted ")
```

```{r}
co2.noaa.weekly.ts.training %>%
  autoplot(.vars = value, color = "red") +
  autolayer(as_tsibble(model.arima.best.NSA$fitted),
            .vars = value,
            color = "blue") +
  labs(title = "NSA In-Sample - Actual vs Fitted ")
```
In the SA and NSA charts, we can see that that our modeled values seems to fit very closely to the actual values. 

Now lets look at out-of-sample accuracy for SA model:
```{r}
#SA
forecast(model.arima.best.SA, h = length(co2.noaa.weekly.ts.SA.test$index) +
           1)$mean %>%
  as_tsibble() %>%
  autoplot(.vars = value, color = "blue") +
  autolayer(co2.noaa.weekly.ts.SA.test,
            .vars = value,
            color = "red") +
  labs(title = "SA : Actual vs Out-Sample Predicted")
```
Seasonally adjusted model predicts the point estimates as a linear model and predicts the pseudo out-sample as a trend line. This is expected as model is based on seasonally adjusted training dataset.

Now lets look at out-of-sample accuracy for NSA model:
```{r}
#NSA
forecast(model.arima.best.NSA, h = length(co2.noaa.weekly.ts.test$index) +
           1)$mean %>%
  as_tsibble() %>%
  autoplot(.vars = value, color = "blue") +
  autolayer(co2.noaa.weekly.ts.test, .vars = value, color = "red") +
  labs(title = "NSA : Actual vs Out-Sample Predicted")
```
Non SA model follows the pseudo out-sample dataset much more closely than SA model. The predictions are smooth over weekly fluctuations but still follows the original time series curve quite closely.

For our pplynomial model, we will use a cubic polynomial with dummy variables for each month since this is the polynomial model variation we previously had the most success with. 
```{r}
#Fit cubic model with month dummy variable
co2.noaa.weekly.ts.SA.training.df <-
  data.frame(
    value = co2.noaa.weekly.ts.SA.training$value,
    time.index = 1:length(co2.noaa.weekly.ts.SA.training$index),
    season = factor(month(co2.noaa.weekly.ts.SA.training$index), ordered = F)
  )
mod.ln.noaa.sa <-
  lm(
    formula = value ~ 0 + time.index + I(time.index ^ 2)  + I(time.index ^ 3) + season,
    data = co2.noaa.weekly.ts.SA.training.df
  )
summary(mod.ln.noaa.sa)
plot(mod.ln.noaa.sa)
```

All of the features are significant in the cubic model. The residual charts suggest a decent fit. Lets check the accuracy of this model:

```{r}
in.sample.accuracy.df = data.frame(
  RMSE = rbind(accuracy(model.arima.best.SA)[, 'RMSE'], accuracy(mod.ln.noaa.sa)[, 'RMSE']),
  row.names = c("Arima-SA", "Linear-SA")
)
in.sample.accuracy.df
```
Accuracy of polynomial model is much worse than Arima model. Lets see how the cubic model performs to pseudo out-sample data:

```{r}
co2.noaa.weekly.ts.SA.test.df <-
  data.frame(
    value = co2.noaa.weekly.ts.SA.test$value,
    time.index = length(co2.noaa.weekly.ts.SA.training$index) + 1:length(co2.noaa.weekly.ts.SA.test$index),
    season = factor(month(co2.noaa.weekly.ts.SA.test$index), ordered = F),
    time.origin = co2.noaa.weekly.ts.SA.test$index
  )

co2.noaa.weekly.ts.SA.test.df$predicted = predict(mod.ln.noaa.sa, newdata = co2.noaa.weekly.ts.SA.test.df)
ggplot(co2.noaa.weekly.ts.SA.test.df, aes(x = time.origin)) +
  geom_line(aes(y = value), color = "red") + #original
  geom_line(aes(y = predicted), color = "blue") + #predicted by polynomial model
  geom_line(aes(y = forecast(
    model.arima.best.SA, h = length(co2.noaa.weekly.ts.SA.test$index) + 1
  )$mean[-1]), color = "cyan") # predicted by SA Arima model
```
In the chart above the red line is the actual data, the blue line is the cubic model, and the cyan line is the Arima model. The polynomial model follows the seasonally adjusted out sample data at the highs of seasons whereas Arima SA model passes through the mean. By simply looking at the chart, it appears the cubic model is a better fit than the Arima SA model. 

**Part 6 (3 points)**

Generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric C02 levels in the year 2100. How confident are you that these are accurate predictions?

Lets first plot forecast for 5500 future intervals which would indicate approximately where 420 and 500 ppm levels were first and last met at 80% and 95% confidence intervals. 
```{r}
forecast(model.arima.best.NSA, h = 5500) %>%
  autoplot() +
  geom_hline(yintercept = 420, color = "red") +
  geom_hline(yintercept = 500, color = "cyan")
```
It appears that using a point estimate, value 420 is first reached in the year 225 and value 500 is first reached at around 2060. We will calculate these years programatically below:
```{r}
# Get forcast for next 5500 weeks, sufficient for lower 95% CI to pass 500 mark
model.arima.best.NSA.predict.ft <-
  forecast(model.arima.best.NSA, h = 5500)

# convert to ts object to find time points in future
model.arima.best.NSA.predict.ts <-
  as_tsibble(
    ts(
      model.arima.best.NSA.predict.ft$mean,
      start = 2017.99905280457 ,
      frequency = 52.18
    )
  )

# find all data point which cross 420 or 500 ppm values
model.arima.best.NSA.predict.tbl <-
  model.arima.best.NSA.predict.ft %>%
  as_tibble() %>%
  mutate(
    PF.420 = `Point Forecast` >= 420,
    Lo95.420 = `Lo 95` >= 420,
    Hi95.420 = `Hi 95` >= 420,
    Lo80.420 = `Lo 80` >= 420,
    Hi80.420 = `Hi 80` >= 420,
    PF.500 = `Point Forecast` >= 500,
    Lo95.500 = `Lo 95` >= 500,
    Hi95.500 = `Hi 95` >= 500,
    Lo80.500 = `Lo 80` >= 500,
    Hi80.500 = `Hi 80` >= 500
  )

findForecastFirst <- function(val) {
  cnt <- model.arima.best.NSA.predict.tbl %>%
    mutate(count = row_number(), first.val = val & !lag(val)) %>%
    filter(first.val) %>%
    head(1) %>%
    select(count)
  
  idx <- model.arima.best.NSA.predict.ts %>%
    select(index) %>%
    filter(row_number() == cnt$count)
  
  idx$index
}

findForecastLast <- function(val) {
  cnt <- model.arima.best.NSA.predict.tbl %>%
    mutate(count = row_number(), first.val = val & !lead(val)) %>%
    filter(first.val) %>%
    tail(1) %>%
    select(count)
  
  idx <- model.arima.best.NSA.predict.ts %>%
    select(index) %>%
    filter(row_number() == cnt$count)
  
  idx$index
}

# 420
PF.420.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$PF.420)
PF.420.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$PF.420)

Lo95.420.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Lo95.420)
Lo95.420.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Lo95.420)

Hi95.420.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Hi95.420)
Hi95.420.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Hi95.420)

Lo80.420.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Lo80.420)
Lo80.420.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Lo80.420)

Hi80.420.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Hi80.420)
Hi80.420.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Hi80.420)

#500
PF.500.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$PF.500)
PF.500.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$PF.500)

Lo95.500.first <- NULL # 95% CI low curve doesn't hit 500 ppm
Lo95.500.last <- NULL # 95% CI low curve doesn't hit 500 ppm

Hi95.500.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Hi95.500)
Hi95.500.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Hi95.500)

Lo80.500.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Lo80.500)
Lo80.500.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Lo80.500)

Hi80.500.first <-
  findForecastFirst(model.arima.best.NSA.predict.tbl$Hi80.500)
Hi80.500.last <-
  findForecastLast(model.arima.best.NSA.predict.tbl$Hi80.500)

predict.420 = list(
  Lo95.420.first,
  Lo95.420.last,
  Lo80.420.first,
  Lo80.420.last,
  PF.420.first,
  PF.420.last,
  Hi80.420.first,
  Hi80.420.last,
  Hi95.420.first,
  Hi95.420.last
)
predict.500 = list(
  Lo95.500.first,
  Lo95.500.last,
  Lo80.500.first,
  Lo80.500.last,
  PF.500.first,
  PF.500.last,
  Hi80.500.first,
  Hi80.500.last,
  Hi95.500.first,
  Hi95.500.last
)
col.names <-
  c(
    "First time lower estimate with 95% CI",
    "Last time lower estimate with 95% CI",
    "First time lower estimate with 80% CI",
    "Last time lower estimate with 80% CI",
    "First time Point Estimate",
    "Last time Point Estimate",
    "First time higher estimate with 80% CI",
    "Last time higher estimate with 80% CI",
    "First time higher estimate with 95% CI",
    "Last time higher estimate with 95% CI"
  )
results.df <- data.frame(predict.420)
results.df[2, ] = predict.500
colnames(results.df) <- col.names
rownames(results.df) <-
  c("Predictions for 420 ppm", "Predictions for 500 ppm")
results.df.t <- t(results.df)

results.df.t[order(results.df.t[, 1]), ]
```
Our programatic approach tells us that week 12 in 2022, the first time point estimate of 420 ppm is reached. The last time point estimate for 420 ppm is in week 30 of 2024. In week 6 of 2057, first time point estimate of 500 ppm is reached and the last time point estimate is week 25 of 2059. 

The higher estimates of the confidence intervals are as follows:
When using a 95% confidence interval, the first time for 420 ppm is in week 8 of 2021 420 ppm and in week 32 of 2022 the last time estimate is reached. Additionally, the first time estimate for 500 ppm is reached in week 51 of 2043 and the last time estimate is reached in week 23 of 2044. Finally, using an 80% confidence interval, in week 12 of 2021 420 ppm is reached, and in week 4 of 2047 500 ppm is reached. 

The lower estimates of the confidence intervals are as follows:
When using a 95% confidence interval, the first time for 420 ppm is in week 11 of 2026 420 ppm and in week 30 of 2030 the last time estimate is reached. With a 95% confidence interval the value 500 ppm is not reached for the low estimate. Finally, using an 80% confidence interval, in week 12 of 2024 420 ppm is reached, and in week 50 of 2101 500 ppm is reached.

Also note that if we keep forcasting any further, the 95% CI forecast turns downwards and may touch 420 ppm again, so above table would change if we take even wider forecast window.

Now lets generate prediction for year 2100:
```{r}
model.arima.best.NSA
co2.noaa.weekly.ts.training
co2.noaa.weekly.ts.2100 = yearweek(seq(as.Date("2100-01-01"), as.Date("2100-12-31"), by = "1 week"))

model.arima.best.NSA.predict.ft %>%
  as_tibble() %>%
  ts(start = 2017.99905280457 , frequency = 52.18) %>%
  as_tsibble() %>%
  filter_index("2100-01-01" ~ .) %>%
  filter_index(. ~ "2100-12-31") %>%
  autoplot(.vars = value) +
  labs(title = "Predicted CO2 levels for year 2100")
```

Point estimate for CO2 levels in year 2100 is around 600 ppm. 80% CI CO2 levels are from 500 ppm to 700 ppm whereas 95 % CI CO2 levels are 450 ppm to 750 ppm.





