---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 2'
subtitle: James Darmody, Maria Vasiliadis and Gaurav Desai
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

```{r warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

# Start with a clean R environment
rm(list = ls())

# Set Fixed random seed to replicate the results
set.seed(28740)

#library(ggplot2)
library(Hmisc)
#library(car)
library(gridExtra)
library(dplyr)
#library(scales)
#library(GGally)
#library(MASS)
#library(purrr)
#library(tidyr)
#library(nnet)
library(purrr)
library(imputeTS)
library(tsibble)
library(fabletools)
library(feasts)
library(fable)
library(lubridate)
library(forecast)
library(seasonal)
library(tsbox)
library(tibble)
library(svMisc)
```

# The Keeling Curve

In the 1950s, the geochemist Charles David Keeling observed a seasonal pattern in the amount of carbon dioxide present in air samples collected over the course of several years. He was able to attribute this pattern to the difference in the amount of land area and vegetation cover between the northern and southern hemipsheres, and the resulting variation in global rates of photosynthesis as the hemispheres' seasons alternated throughout the year. 

In 1958 Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present.

The `co2` data set in R's `datasets` package (automatically loaded with base R) is a monthly time series of atmospheric carbon dioxide concentrations measured in ppm (parts per million) at the Mauna Loa Observatory from 1959 to 1997. The curve graphed by this data is known as the 'Keeling Curve'.

```{r}
plot(co2, ylab = expression("CO2 ppm"), col = 'blue', las = 1)
title(main = "Monthly Mean CO2 Variation")
```

\newpage

**Part 1 (4 points)**

Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include thorough analyses of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed.

**Answer**
Lets look at the data
```{r}
head(co2)
tail(co2)
str(co2)
summary(co2)
hist(co2)
```
The co2 dataset is comprised of 468 observations of data from 1959 to 1998 where each datapoint is one month's data. The co2 levels in the dataset range from between 313.2 to 366.8. The histogram shows us that the most common co2 levels are between 320 and 325, and generally the frequency of co2 values decreases as co2 goes up. There is a small spike of frequencies between co2 level 355 and 360. There are no missing values in the dataset. Histogram of value is left skewed but that may not be correct representation of the co2 emission levels over time. So lets look at it using time dimension.

```{r}
co2.ts <- as_tsibble(co2)
autoplot(co2.ts,.vars = value)
```
We observe two things
1. There is upward trend year on year in CO2 emissions
2. There is a seasonal fluctuation within a year

Lets try to decompose and see the individual components in more detail

```{r}
co2.ts %>% model(STL(value)) %>% components() %>% autoplot()
```
When we decompose using STL model, we can see the upward trend separated from yearly seasonal cycles.
We note that seasonal cycle ups and downs are growing with time, so we may need to stabilize the seasons for better predictions.


```{r}
co2.ts %>% gg_tsdisplay(plot_type = "partial", y = value)
```

The ACF tells us that there is autcorrelation even after 24 months. On the other hand PACF drops after 2 but again pops up at multples of 12 months. This indicates that the model could be AR model with 12 months seasonal cycles.

No lets look at each component one by one starting with trend

```{r}
co2.ts.components <- co2.ts %>% model(STL(value)) %>% components() 
ggplot(data=co2.ts.components)+
  geom_line(aes(x=index, y=trend)) +
  ggtitle("Trend")
```
There are minor bumps and dips but there is no major shock or sudden movement in trend.

Now lets look at seasonal cycles.

```{r}
co2.ts %>%
  gg_season(y=value, period = "year")+
  ylab("CO2 emission")+
    ggtitle("Seasonal plot : Monthly CO2 emission")

ggseasonplot(as.ts(co2.ts), year.labels=FALSE, continuous=TRUE, polar = TRUE) #Looks good may be?
```
We can clearly see that CO2 emission is higher in April-May and goes down in September-October every year. This could be due to summer in Northern hemisphere vs southern hemisphere. Population, vegitation and other attributes of these two halves of earth are quite different which could potentially cause this seasonal effect.

Now lets look at same data from another angle.
```{r}
co2.ts %>%
  gg_subseries(y=value, period = "year")+
  geom_hline(aes(yintercept=mean(co2.ts$value)), colour="red")+
  ylab("CO2 emission")+
  xlab("Years")+
  ggtitle("Seasonal subseries plot : Monthly CO2 emission")
```
It is more or less same observation that northern summer increases CO2 emission and southern summer decreases CO2 eission within a year. But we can cearly see that CO2 emission is on the rise year on year for each and every month.

Since we saw earlier that seasonal patterns are are increasin in variance, we can try to transform it so the seasonal changes are uniform across time series. This is desirable property for finding a good model fit.

```{r}
lambda <- co2.ts %>%
  features(value, features=guerrero) %>%
  pull(lambda_guerrero)

co2.ts.trans <- as_tsibble(ts( box_cox(co2.ts$value, lambda), start = c(1959,1)))
co2.ts.trans.comp <- co2.ts %>% model(STL(box_cox(value, lambda))) 

co2.ts.trans.comp %>% components() %>% autoplot() +
  ggtitle(paste("Box-Cox Tranformed Decompositions for lambda=",round(lambda,3)))
```
After transformation we can see the seasonal pattern variance remains constant throughout.


Now lets look at residuals

```{r}

# Lets look at residuals

#Before box-cox transformation
co2.ts.components  %>% select("remainder") %>%
  ggplot(aes(x=remainder)) +geom_histogram()

#After box-cox transformation
co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  ggplot(aes(x=remainder)) +geom_histogram()


```
Both before and after box-cox transformation residuals look fairly normally distributed

```{r}

co2.ts.components %>% select("remainder") %>%
  autoplot()

co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  autoplot()
  
```

Residuals appear to be stationary and appear to be white noise with no trend or seasonal patterns. 

```{r}
co2.ts.components %>% select("remainder") %>%
  gg_tsdisplay(plot_type = "partial")

co2.ts.trans.comp %>% components() %>% select("remainder") %>%
  gg_tsdisplay(plot_type = "partial")
```

**Part 2 (3 points)**

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a suitable polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. 

Lets first fir a linear time trend model

```{r}
time.index <- 1:length(co2.ts$index)
mod.ln.1 <- lm(formula = value ~ time.index, data=co2.ts)
summary(mod.ln.1)
plot(mod.ln.1)

```

The residuals show a pattern indicating linear model is not able to capture the curvature of the trend. Lets try quadratic term

```{r}
mod.ln.2 <- lm(formula = value ~ time.index + I(time.index^2), data=co2.ts)
summary(mod.ln.2)
plot(mod.ln.2)

```

The residuals are showing a much better fit. Now lets try cubic term and see if we get any more improvements.

```{r}
mod.ln.3 <- lm(formula = value ~ time.index + I(time.index^2) + I(time.index^3), data=co2.ts)
summary(mod.ln.3)
plot(mod.ln.3)

```
This model is even better fit, but Q-Q plot shows deviation at start and end. So lets try taking log of value and see if that scale fit any better.


```{r}
mod.ln.4 <- lm(formula = log(value) ~ time.index + I(time.index^2)+ I(time.index^3), data=co2.ts)
summary(mod.ln.4)
plot(mod.ln.4)

```

Taking log doesn't improve much.

So finally lets choose the best of the models, which so far is cubic model,  and add seasonal dummy variables for Jan to December

```{r}
co2.df <- data.frame(value=co2.ts$value, time.index=time.index, season=factor(time.index%%12, ordered = F))
mod.ln.5 <- lm(formula = value ~ 0 + time.index + I(time.index^2)  + I(time.index^3) + season, data=co2.df)
summary(mod.ln.5)
plot(mod.ln.5)


```

This model has better R-square than any of the previous models. Lets use this model to forecast for year 2020

```{r}
time.index.2020 = seq(from=(2020-1959)*12+1,length.out=12)
co2.2020.df <- data.frame(time.index= time.index.2020, season=factor(time.index.2020%%12, ordered = F))

co2.2020.pred <- predict(object = mod.ln.5, newdata = co2.2020.df)

co2.2020.pred.ts <- as_tsibble(ts(data=co2.2020.pred, start = c(2020,1), frequency=12))

co2.2020.pred.ts %>%
  autoplot(.vars = value)

```

**Part 3 (3 points)**

Following all appropriate steps, choose an ARIMA model to fit to the series. Discuss the characteristics of your model and how you selected between alternative ARIMA specifications. Write your model (or models) using backshift notation. Use your model (or models) to generate forecasts to the year 2020. 

```{r}
no.p=no.P=2
no.q=no.Q=2
no.d=no.D=1
no.of.models<-(no.p+1)*(no.d+1)*(no.q+1)*(no.P+1)*(no.D+1)*(no.Q+1)

print(paste("Number of models to fit =",no.of.models))

params.df <- expand.grid(p=0:no.p, d=0:no.d, q=0:no.q, P=0:no.P, D=0:no.D, Q=0:no.Q)
i <- 1

funFitModel <- function(param.row){
  progress(value=i, max.value = no.of.models, console = TRUE, progress.bar = TRUE)
  i <- i+1
  p = param.row['p']
  q = param.row['q']
  d = param.row['d']
  P = param.row['P']
  Q = param.row['Q']
  D = param.row['D']
  
  #print(paste(p,q,d,P,Q,D))
  tryCatch({
  model.fit = Arima(y=as.ts(co2.ts), order=c(p,d,q), seasonal=c(P,D,Q), lambda=lambda, include.drift = FALSE);
  model.info = data.frame(p,q,d,P,Q,D, model.fit$aic, model.fit$aicc, model.fit$bic);

  return (model.info);
  }, error=function(e){
    return (data.frame())
  })
  
}

model.fit.info.df <- do.call("rbind",(apply(params.df, 1,funFitModel )))
colnames(model.fit.info.df) <- c("p","q", "d", "P", "Q", "D", "aic", "aicc", "bic")
print("Top 6 models by BIC")
model.fit.info.df %>%
  arrange(bic) %>%
  head()

```

Using BIC as criteria we can see that ARIMA (0,1,1) with seasonality (2,0,1) as best model for CO2 time series. BIC for this model is least -5389.765. Note that this model uses lambda= -0.03432107 and no drift.

Lets just save the model to forecast for year 2020

```{r}
model.arima.best.q3 = Arima(y=as.ts(co2.ts), order=c(0,1,1), seasonal=c(2,0,1), lambda=lambda, include.drift = FALSE);
co2.2020.arima.pred.ts <- forecast(model.arima.best.q3, h=276) %>%
  as_tibble() %>%
  dplyr::select('Point Forecast') %>%
  ts(start = c(1998,1), frequency = 12) %>%
  as_tsibble() %>%
  filter_index("2019-12-31" ~.) 

co2.2020.arima.pred.ts %>%
  autoplot(.vars = value) +
  labs(title="2020 CO2 predictions from best fitted ARIMA (0,1,1) (2,0,1) model")
```
Below, we inscribe the model using Latex and Backshift notation.

$$
  x_{t}(1-B) - \alpha B^{12}x_{t}(1-B) = Bw_{t}(1-\beta B^{12})
$$


**Part 4 (4 points)**

The file `co2_weekly_mlo.txt` contains weekly observations of atmospheric carbon dioxide concentrations measured at the Mauna Loa Observatory from 1974 to 2020, published by the National Oceanic and Atmospheric Administration (NOAA). Convert these data into a suitable time series object, conduct a thorough EDA on the data, and address the problem of missing observations. Describe how the Keeling Curve evolved from 1997 to the present and compare current atmospheric CO2 levels to those predicted by your forecasts in Parts 2 and 3. Use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2 and 3 over the entire period.  

```{r}
co2.noaa.weekly.df <- read.table('co2_weekly_mlo.txt', header = FALSE, comment.char='#', na.strings = '-999.99')
colnames(co2.noaa.weekly.df) <- c('year','month','day','decimal.day', 'record.value', '#days', 'one.year.ago','ten.years.ago','since.1800.diff')
```

```{r}
head(co2.noaa.weekly.df)
tail(co2.noaa.weekly.df)
```
```{r}
summary(co2.noaa.weekly.df) # 20 readings are missing
describe(co2.noaa.weekly.df)
co2.noaa.weekly.df <- co2.noaa.weekly.df %>%
  mutate(record.date=ymd(paste(year,month,day)))
```
We can see in the dataset there are 2388 observations. The make the time series easier to work with, we create an index using all of the time columns and convert the data to a tsibble object. 

```{r}
week.frequency = 365.25/7
co2.noaa.weekly.ts.raw <- ts(data=co2.noaa.weekly.df$record.value,start = 1974.3795	, frequency = week.frequency)
co2.noaa.weekly.ts <- as_tsibble(ts(data=co2.noaa.weekly.df$record.value,start = 1974.3795	, frequency = week.frequency), class="matrix")
cbind(head(co2.noaa.weekly.ts),tail(co2.noaa.weekly.ts))
summary(co2.noaa.weekly.ts)
class(co2.noaa.weekly.ts)
```
In the summary of the tsibble object, we can see that there are 20 missing values. 

To fill in the missing data, we fill in the values with interpolation from neighbour data points. We can try and fit a model and use that model instead for interpolation. After the interpolation we can see that there are 0 rows with missing values. 
```{r}
co2.noaa.weekly.ts%>%
  filter(is.na(value))

co2.noaa.weekly.ts <- na_interpolation(co2.noaa.weekly.ts, option = "spline")

co2.noaa.weekly.ts%>%
  filter(is.na(value))

```

Plot Keeling curve
```{r}
co2.noaa.weekly.ts %>%
  autoplot(.vars = value) +
  geom_smooth() +
  labs(title = "Keeling Curve till 2020", y="Co2 emission (units)", x="Year")

co2.ts %>%
  gg_season(y=value, period = "year")+
  ylab("CO2 emission")+
    ggtitle("Seasonal plot : Monthly CO2 emission")

  
```

We can see that from about 1995 onwards the trend has picked up and increasing faster than pre 1995 trend.

```{r}
#Additional EDA
co2.noaa.weekly.ts %>%
  gg_season(y=value, period = "year")+
  ylab("CO2 emission")+
    ggtitle("Seasonal plot : Monthly CO2 emission")

```
The seasonal plot looks similar to the monthly CO2 data evaluated earlier, but we can now see the effects of the weekly data points by the variation in each line versus the smoother line in the monthly data. The seasonal monthly trends, however, are clearly the same. 

Now lets compare the current CO2 levels to predicted from Q2 and 3

```{r}
co2.noaa.weekly.ts %>%
  filter_index("2019-12-31" ~.) %>%
  mutate(index = as.Date(index)) %>%
  autoplot(.vars = value, color="blue") +
  autolayer(co2.2020.pred.ts, .vars = value, color="red") +
  autolayer(co2.2020.arima.pred.ts, .vars = value, color="navy") +
  labs(title="Year 2020 Actual vs Prediction from Linear Model and ARIMA model")

```
The predicted CO2 levels are lesser compared to actual levels. The red line is the predicted values from the cubic model with seasonal variables and the navy line is the predicted values from the ARIMA model. 

Now lets compare monthly average curve with model forecasts
```{r}
#First lets convert to monthly time series
co2.noaa.monthly.ts <- co2.noaa.weekly.ts %>%
  as_tibble() %>%
  mutate(yearmonth=yearmonth(yearweek(index))) %>%
  group_by(yearmonth) %>%
  summarise(avg_value=mean(value)) %>%
  as_tsibble(index = yearmonth, value=avg_value) 

summary(co2.noaa.monthly.ts) #> 1974 May to 2020 Feb


# Now predict using linear model for same time frame
time.index.1974.may = seq(from=(1974-1959)*12+5,length.out=550)
co2.noaa.like.df <- data.frame(time.index= time.index.1974.may, season=factor(time.index.1974.may%%12, ordered = F))
co2.noaa.like.pred <- predict(object = mod.ln.5, newdata = co2.noaa.like.df)
co2.noaa.like.pred.ts <- as_tsibble(ts(data=co2.noaa.like.pred, start = c(1974,5), frequency=12))


# now predict using Arima model selected in Q3
co2.noaa.arima.pred.ts <- forecast(model.arima.best.q3, h=276) %>%
  as_tibble() %>%
  dplyr::select('Point Forecast') %>%
  ts(start = c(1998,1), frequency = 12) %>%
  as_tsibble() 

model.arima.best.q3.fitted.ts <-model.arima.best.q3$fitted %>%
  as_tsibble() 


co2.noaa.monthly.ts %>%
  autoplot(.vars = avg_value, color="blue") +
  autolayer(co2.noaa.like.pred.ts, .vars=value, color="red") +
  autolayer(co2.noaa.arima.pred.ts, .vars=value, color="navy") +
  autolayer(model.arima.best.q3.fitted.ts, .vars=value, color="navy") +
  labs(title="Part 2 and 3 model predictions vs NOAA monthly average")
```
We see that up until about the year 2000, both models model the actual data closely. After, the models separate from the actuals with the cubic model showing the lowest values (red), and the ARIMA model (navy) also showing lower values than the actual values (blue). 

**Part 5 (3 points)**

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

First lets try couple of ways in which we can decompose seasonality by analysing residual
```{r}
plotRemainderFromSTL <- function(s.degree=0){
  stl.val <- co2.noaa.weekly.ts %>%
    stl(s.window = week.frequency, s.degree = s.degree)
  ts.obj <- as_tsibble(ts(stl.val$time.series[,'remainder'],start = 1974.3795	, frequency = week.frequency)) 
  
  par(mfrow=c(1,2))
  #p1 <- ts.obj %>% autoplot(.vars = value)
  ts.obj %>% gg_tsdisplay(y = value,plot_type = "partial")

}


plotRemainderFromSTL(s.degree=0)
plotRemainderFromSTL(s.degree=1)
```

We can see that with degree of locality 1 in seasonal extraction gives is much better residuals with dampening ACF and sharp decrease in PACF at lag 1. There is still some seasonality left but still this looks much better than degree 0.

Lets first see how seasonal decomposition looks
```{r}
co2.noaa.weekly.ts %>%
  stl(s.window = week.frequency, s.degree = 1) %>%
  autoplot()
```
Now lets substract seasonal data from time series to get seasonally adjusted time series
```{r}
co2.noaa.weekly.ts.components <- stl(co2.noaa.weekly.ts ,s.window = week.frequency)$time.series
co2.noaa.weekly.ts.components.seasonal <- co2.noaa.weekly.ts.components[,'seasonal']
co2.noaa.weekly.ts.SA <- as_tsibble(ts(co2.noaa.weekly.ts$value - co2.noaa.weekly.ts.components.seasonal,start = 1974.3795	, frequency = week.frequency))
cbind(head(co2.noaa.weekly.ts),tail(co2.noaa.weekly.ts),head(co2.noaa.weekly.ts.SA),tail(co2.noaa.weekly.ts.SA))
```


Lets check the SA time series
```{r}
co2.noaa.weekly.ts.SA %>%
  autoplot()

co2.noaa.weekly.ts.SA %>%  gg_tsdisplay(plot_type = "partial")
```
We can see a smoother trend line with the seasonally adjusted series. The acf chart shows a very slow decreasing lag for every  30+ lags. The pacf chart suggests a non-stationary random walk process. 

Now we split the seasonally adjusted and non-seasonally adjusted time series into training and test sets. With 2388 obversations, we can take the last 2 years of the dataset by extracting the last 104 observations, since every observation is 1 week. The remaining 2284 observations are the training set. 
```{r}
trainTestSplit <- function(time.series){
  time.series.test <- time.series %>%
  filter_index("2018-01-01" ~.) 
  
  time.series.train <- time.series %>%
  filter_index(.~"2017-12-31")
  
  return (list(train=time.series.train,test=time.series.test))
}

#Split SA data into training and test

co2.noaa.weekly.ts.SA.train.test <- trainTestSplit(co2.noaa.weekly.ts.SA)
co2.noaa.weekly.ts.SA.test <- co2.noaa.weekly.ts.SA.train.test$test
co2.noaa.weekly.ts.SA.training <- co2.noaa.weekly.ts.SA.train.test$train

#Split NSA data into training and test
co2.noaa.weekly.ts.train.test <- trainTestSplit(co2.noaa.weekly.ts)
co2.noaa.weekly.ts.test <- co2.noaa.weekly.ts.train.test$test
co2.noaa.weekly.ts.training <- co2.noaa.weekly.ts.train.test$train
```

Now lets fit Arima model for SA and non SA time serieses.


First lets start with SA time sries
```{r}
#Fit ARIMA model for SA data
no.p=3
no.d=2
no.q=3

# P,D, Q are set to 0 because training data is already seasonally adjusted
no.P=0 
no.D=0 
no.Q=0 

no.of.models<-(no.p+1)*(no.d+1)*(no.q+1)*(no.P+1)*(no.D+1)*(no.Q+1)

print(paste("Number of models to fit =",no.of.models))

params.df <- expand.grid(p=0:no.p, d=0:no.d, q=0:no.q, P=0:no.P, D=0:no.D, Q=0:no.Q)
i <- 1

funFitModel <- function(param.row){
  progress(value=i, max.value = no.of.models, console = TRUE, progress.bar = TRUE)
  i <- i+1
  p = param.row['p']
  d = param.row['d']
  q = param.row['q']
  P = param.row['P']
  D = param.row['D']
  Q = param.row['Q']
  
  #print(paste(p,d,q,P,D,Q))
  tryCatch({
  model.fit = Arima(y=as.ts(co2.noaa.weekly.ts.SA.training), order=c(p,d,q), seasonal=c(0,0,0), include.drift = FALSE);
  model.info = data.frame(p,d,q,P,D,Q, model.fit$aic, model.fit$aicc, model.fit$bic);

  return (model.info);
  }, error=function(e){
    return (data.frame())
  })
  
}

model.fit.info.df.SA <- do.call("rbind",(apply(params.df, 1,funFitModel )))
colnames(model.fit.info.df.SA) <- c("p","d", "q", "P", "D", "Q", "aic", "aicc", "bic")
```
```{r}
print("Top 6 models by BIC")
model.fit.info.df.SA %>%
  arrange(bic) %>%
  head()
```

Arima(0,2,3) is in top 3 models by all 3 measres AIC, AICc and BIC. We pick this model as best model as this has least number of parameters compared to other top models.

```{r}
model.arima.best.SA = Arima(y=as.ts(co2.noaa.weekly.ts.SA.training), order=c(0,2,3), seasonal=c(0,0,0), include.drift = FALSE);
summary(model.arima.best.SA)
checkresiduals(model.arima.best.SA)
```

Now lets fit Arima model to non SA time series.

```{r}
#Fit ARIMA model for NSA data
no.p=no.P=1
no.q=no.Q=1
no.d=no.D=1
no.of.models<-(no.p+1)*(no.d+1)*(no.q+1)*(no.P+1)*(no.D+1)*(no.Q+1)

print(paste("Number of models to fit =",no.of.models))

params.df <- expand.grid(p=0:no.p, d=0:no.d, q=0:no.q, P=0:no.P, D=0:no.D, Q=0:no.Q)
i <- 1

funFitModel <- function(param.row){
  progress(value=i, max.value = no.of.models, console = TRUE, progress.bar = TRUE)
  i <- i+1
  p = param.row['p']
  d = param.row['d']
  q = param.row['q']
  P = param.row['P']
  D = param.row['D']
  Q = param.row['Q']
  
  #print(paste(p,q,d,P,Q,D))
  tryCatch({
  model.fit = Arima(y=as.ts(co2.noaa.weekly.ts.training), order=c(p,d,q), seasonal=c(P,D,Q),include.drift = FALSE);
  model.info = data.frame(p,d,q,P,D,Q, model.fit$aic, model.fit$aicc, model.fit$bic);

  return (model.info);
  }, error=function(e){
    return (data.frame())
  })
  
}

model.fit.info.df.NSA <- do.call("rbind",(apply(params.df, 1,funFitModel )))
colnames(model.fit.info.df.NSA) <- c("p","d", "q", "P", "D", "Q", "aic", "aicc", "bic")
```
```{r}
print("Top 6 models by BIC")
model.fit.info.df.NSA %>%
  arrange(bic) %>%
  head()
```

Best model for NSA time sries is Arima(1,1,1)(0,1,1). We selected this model because has lowest AIC, AICc and BIC so this is natural selection.
We can try with higher values of p,d and q if but it is going to take much longer to estimate the models as number of parameters quickly increase exponentially.

```{r}
model.arima.best.NSA = Arima(y=as.ts(co2.noaa.weekly.ts.training), order=c(1,1,1), seasonal=c(0,1,1), include.drift = FALSE);
summary(model.arima.best.NSA)
checkresiduals(model.arima.best.NSA)
```

Now lets compare how these selected model perform in-sample and pseudo out-of-sample data

```{r}
# in-sample
in.sample.accuracy = data.frame(rbind(accuracy(model.arima.best.SA), accuracy(model.arima.best.NSA)), row.names = c("SA", "NSA"))
in.sample.accuracy
```
If we look at the in-sample accuracy of our SA and NSA best models then SA model marginal does better than NSA but there is not much to choose.
Lets look at actual vs fitted values in chart.

```{r}
co2.noaa.weekly.ts.SA.training %>%
  autoplot(.vars = value, color="red") +
  autolayer(as_tsibble(model.arima.best.SA$fitted), .vars = value, color="blue") +
  labs(title = "SA In-Sample - Actual vs Fitted ")
```

```{r}
co2.noaa.weekly.ts.training %>%
  autoplot(.vars = value, color="red") +
  autolayer(as_tsibble(model.arima.best.NSA$fitted), .vars = value, color="blue") +
  labs(title = "NSA In-Sample - Actual vs Fitted ")
```

Now lets look at Out-sample accuracy

For SA model
```{r}

#SA - 

forecast(model.arima.best.SA, h=length(co2.noaa.weekly.ts.SA.test$index)+1 )$mean %>%
  as_tsibble() %>%
  autoplot(.vars = value, color="blue") +
  autolayer(co2.noaa.weekly.ts.SA.test,.vars = value, color="red")+
  labs(title = "SA : Actual vs Out-Sample Predicted")

```
Seasonally adjusted model predicts the point estimates as a linear model and predicts the pseudo out-saple as a trend line. This is expected as model is based on seasonally adjusted training dataset.


For NSA model
```{r}

#SA - 

forecast(model.arima.best.NSA, h=length(co2.noaa.weekly.ts.test$index)+1 )$mean %>%
  as_tsibble() %>%
  autoplot(.vars = value, color="blue") +
  autolayer(co2.noaa.weekly.ts.test,.vars = value, color="red")+
  labs(title = "NSA : Actual vs Out-Sample Predicted")


```
Non SA model follows the pseudo out-sample dataset much more closely than SA model. The predictions are smooth over weekly fluctuations but still follows the original time series curve quite closely.


For our pplynomial model, we will use a cubic polynomial with dummy variables for each month since this is the polynomial model variation we previously had the most success with. 
```{r}
#Fit cubic model with month dummy variable

co2.noaa.weekly.ts.SA.training.df <- data.frame(value=co2.noaa.weekly.ts.SA.training$value, time.index=1:length(co2.noaa.weekly.ts.SA.training$index), season=factor(month(co2.noaa.weekly.ts.SA.training$index), ordered = F))
mod.ln.noaa.sa <- lm(formula = value ~ 0 + time.index + I(time.index^2)  + I(time.index^3) + season, data=co2.noaa.weekly.ts.SA.training.df)
summary(mod.ln.noaa.sa)
plot(mod.ln.noaa.sa)
```

Linear model all the features as significant. Lets check the accuracy of this model

```{r}
in.sample.accuracy.df = data.frame(RMSE=rbind(accuracy(model.arima.best.SA)[,'RMSE'], accuracy(mod.ln.noaa.sa)[,'RMSE']), row.names = c("Arima-SA", "Linear-SA"))
in.sample.accuracy.df
```
Accuracy of polinomial model is much better than Arima model. Lets see how this model performs to pseudo out-sample data

```{r}
co2.noaa.weekly.ts.SA.test.df <- data.frame(value=co2.noaa.weekly.ts.SA.test$value, time.index=length(co2.noaa.weekly.ts.SA.training$index)+1:length(co2.noaa.weekly.ts.SA.test$index), season=factor(month(co2.noaa.weekly.ts.SA.test$index), ordered = F),time.origin=co2.noaa.weekly.ts.SA.test$index)

co2.noaa.weekly.ts.SA.test.df$predicted = predict(mod.ln.noaa.sa, newdata = co2.noaa.weekly.ts.SA.test.df)
ggplot(co2.noaa.weekly.ts.SA.test.df, aes(x=time.origin))+
  geom_line(aes(y=value), color="red")+ #original
  geom_line(aes(y=predicted), color="blue")+ #prediected by polynomial model
  geom_line(aes(y=forecast(model.arima.best.SA, h=length(co2.noaa.weekly.ts.SA.test$index)+1 )$mean[-1]), color="cyan") # predicted by SA Arima model
```
The polynomial model follows the seasonally adjusted out sample data better than Arima model fitted on same training dataset.

**Part 6 (3 points)**

Generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric C02 levels in the year 2100. How confident are you that these are accurate predictions?










