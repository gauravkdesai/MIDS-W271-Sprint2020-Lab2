---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 2'
subtitle: James Darmody, Maria Vasiliadis and Gaurav Desai
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

```{r warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

# Start with a clean R environment
rm(list = ls())

# Set Fixed random seed to replicate the results
set.seed(28740)

#library(ggplot2)
library(Hmisc)
#library(car)
#library(gridExtra)
library(dplyr)
#library(scales)
#library(GGally)
#library(MASS)
#library(purrr)
#library(tidyr)
#library(nnet)
library(purrr)
library(imputeTS)
library(tsibble)
library(fabletools)
library(feasts)
library(fable)
library(lubridate)
library(forecast)
library(seasonal)
library(tsbox)
library(tibble)
library(svMisc)
```

# The Keeling Curve

In the 1950s, the geochemist Charles David Keeling observed a seasonal pattern in the amount of carbon dioxide present in air samples collected over the course of several years. He was able to attribute this pattern to the difference in the amount of land area and vegetation cover between the northern and southern hemipsheres, and the resulting variation in global rates of photosynthesis as the hemispheres' seasons alternated throughout the year. 

In 1958 Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present.

The `co2` data set in R's `datasets` package (automatically loaded with base R) is a monthly time series of atmospheric carbon dioxide concentrations measured in ppm (parts per million) at the Mauna Loa Observatory from 1959 to 1997. The curve graphed by this data is known as the 'Keeling Curve'.

```{r}
plot(co2, ylab = expression("CO2 ppm"), col = 'blue', las = 1)
title(main = "Monthly Mean CO2 Variation")
```

\newpage

**Part 1 (4 points)**

Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include thorough analyses of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed.

**Answer**
Lets look at the data
```{r}
head(co2)
tail(co2)
str(co2)
summary(co2)
hist(co2)
```

There are no missing values. Histogram of value is left skewed but that may not be correct representation of the co2 emission levels over time. So lets look at it using time dimension.

```{r}
co2.ts <- as_tsibble(co2)
autoplot(co2.ts,.vars = value)
```
We observe two things
1. There is upward trend year on year in CO2 emissions
2. There is a seasonal fluctuation within a year

Lets try to decompose and see the individual components in more detail

```{r}
co2.ts %>% model(STL(value)) %>% components() %>% autoplot()
```
When we decompose using STL model, we can see the upward trend separated from yearly seasonal cycles.
We note that seasonal cycle ups and downs are growing with time, so we may need to stabilize the seasons for better predictions.


```{r}
co2.ts %>% gg_tsdisplay(plot_type = "partial", y = value)
```

The ACF tells us that there is autcorrelation even after 24 months. On the other hand PACF drops after 2 but again pops up at multples of 12 months.
This indicates that the model could be AR model with 12 months seasonal cycles.

No lets look at each component one by one starting with trend

```{r}
co2.ts.components <- co2.ts %>% model(STL(value)) %>% components() 
ggplot(data=co2.ts.components)+
  geom_line(aes(x=index, y=trend)) +
  ggtitle("Trend")
```
There are minor bumps and dips but there is no major shock or sudden movement in trend.

Now lets look at seasonal cycles.

```{r}
co2.ts %>%
  gg_season(y=value, period = "year")+
  ylab("CO2 emission")+
    ggtitle("Seasonal plot : Monthly CO2 emission")

ggseasonplot(as.ts(co2.ts), year.labels=FALSE, continuous=TRUE, polar = TRUE) #Looks good may be?
```
We can clearly see that CO2 emission is higher in April-May and goes down in September-October every year. This could be due to summer in Northern hemisphere vs southern hemisphere. Population, vegitation and other attributes of these two halves of earth are quite different which could potentially cause this seasonal effect.

Now lets look at same data from another angle.
```{r}
co2.ts %>%
  gg_subseries(y=value, period = "year")+
  geom_hline(aes(yintercept=mean(co2.ts$value)), colour="red")+
  ylab("CO2 emission")+
  xlab("Years")+
  ggtitle("Seasonal subseries plot : Monthly CO2 emission")
```
It is more or less same observation that northern summer increases CO2 emission and southern summer decreases CO2 eission within a year. But we can cearly see that CO2 emission is on the rise year on year for each and every month.

Since we saw earlier that seasonal patterns are are increasin in variance, we can try to transform it so the seasonal changes are uniform across time series. This is desirable property for finding a good model fit.

```{r}
lambda <- co2.ts %>%
  features(value, features=guerrero) %>%
  pull(lambda_guerrero)

co2.ts.trans <- as_tsibble(ts( box_cox(co2.ts$value, lambda), start = c(1959,1)))
co2.ts.trans.comp <- co2.ts %>% model(STL(box_cox(value, lambda))) 

co2.ts.trans.comp %>% components() %>% autoplot() +
  ggtitle(paste("Box-Cox Tranformed Decompositions for lambda=",round(lambda,3)))
```
After transformation we can see the seasonal pattern variance remains constant throughout.


Now lets look at residuals

```{r}

# Lets look at residuals

#Before box-cox transformation
co2.ts.components  %>% select("remainder") %>%
  ggplot(aes(x=remainder)) +geom_histogram()

#After box-cox transformation
co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  ggplot(aes(x=remainder)) +geom_histogram()


```
Both before and after box-cox transformation residuals look fairly normally distributed

```{r}

co2.ts.components %>% select("remainder") %>%
  autoplot()

co2.ts.trans.comp  %>% components() %>% select("remainder") %>%
  autoplot()
  
```

Residuals appear to be white noise with no trend or seasonal patterns

```{r}

co2.ts.components %>% select("remainder") %>%
  gg_tsdisplay(plot_type = "partial")

co2.ts.trans.comp %>% components() %>% select("remainder") %>%
  gg_tsdisplay(plot_type = "partial")
```

**Part 2 (3 points)**

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a suitable polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. 

Lets first fir a linear time trend model

```{r}
time.index <- 1:length(co2.ts$index)
mod.ln.1 <- lm(formula = value ~ time.index, data=co2.ts)
summary(mod.ln.1)
plot(mod.ln.1)

```

The residuals show a pattern indicating linear model is not able to capture the curvature of the trend. Lets try quadratic term

```{r}
mod.ln.2 <- lm(formula = value ~ time.index + I(time.index^2), data=co2.ts)
summary(mod.ln.2)
plot(mod.ln.2)

```

The residuals are showing much better fitment. Now lets try cubic term and see if we get any more improvements.

```{r}
mod.ln.3 <- lm(formula = value ~ time.index + I(time.index^2) + I(time.index^3), data=co2.ts)
summary(mod.ln.3)
plot(mod.ln.3)

```
This model is even better fit, but Q-Q plot shows deviation at start and end. So lets try taking log of value and see if that scale fit any better.


```{r}
mod.ln.4 <- lm(formula = log(value) ~ time.index + I(time.index^2)+ I(time.index^3), data=co2.ts)
summary(mod.ln.4)
plot(mod.ln.4)

```

Taking log doesn't improve much.


So finally lets choose the best of the models so far which is cubic model and add seasonal dummy variables for Jan to December

```{r}
co2.df <- data.frame(value=co2.ts$value, time.index=time.index, season=factor(time.index%%12, ordered = F))
mod.ln.5 <- lm(formula = value ~ 0 + time.index + I(time.index^2)  + I(time.index^3) + season, data=co2.df)
summary(mod.ln.5)
plot(mod.ln.5)


```

This model has better R-square than any of the previous models. Lets use this model to forcast for year 2020

```{r}
time.index.2020 = seq(from=(2020-1959)*12+1,length.out=12)
co2.2020.df <- data.frame(time.index= time.index.2020, season=factor(time.index.2020%%12, ordered = F))

co2.2020.pred <- predict(object = mod.ln.5, newdata = co2.2020.df)

co2.2020.pred.ts <- as_tsibble(ts(data=co2.2020.pred, start = c(2020,1), frequency=12))

co2.2020.pred.ts %>%
  autoplot(.vars = value)

```

**Part 3 (3 points)**

Following all appropriate steps, choose an ARIMA model to fit to the series. Discuss the characteristics of your model and how you selected between alternative ARIMA specifications. Write your model (or models) using backshift notation. Use your model (or models) to generate forecasts to the year 2020. 

```{r}
no.p=no.P=2
no.q=no.Q=2
no.d=no.D=1
no.of.models<-(no.p+1)*(no.d+1)*(no.q+1)*(no.P+1)*(no.D+1)*(no.Q+1)

print(paste("Number of models to fit =",no.of.models))

params.df <- expand.grid(p=0:no.p, d=0:no.d, q=0:no.q, P=0:no.P, D=0:no.D, Q=0:no.Q)
i <- 1

funFitModel <- function(param.row){
  progress(value=i, max.value = no.of.models, console = TRUE, progress.bar = TRUE)
  i <- i+1
  p = param.row['p']
  q = param.row['q']
  d = param.row['d']
  P = param.row['P']
  Q = param.row['Q']
  D = param.row['D']
  
  #print(paste(p,q,d,P,Q,D))
  tryCatch({
  model.fit = Arima(y=as.ts(co2.ts), order=c(p,d,q), seasonal=c(P,D,Q), lambda=lambda, include.drift = FALSE);
  model.info = data.frame(p,q,d,P,Q,D, model.fit$aic, model.fit$aicc, model.fit$bic);

  return (model.info);
  }, error=function(e){
    return (data.frame())
  })
  
}

model.fit.info.df <- do.call("rbind",(apply(params.df, 1,funFitModel )))
colnames(model.fit.info.df) <- c("p","q", "d", "P", "Q", "D", "aic", "aicc", "bic")
print("Top 6 models by BIC")
model.fit.info.df %>%
  arrange(bic) %>%
  head()



```

Using BIC as criteria we can see that ARIMA (0,1,1) with seasonality (2,0,1) as best model for CO2 time series. BIC for this model is least -5389.765. Note that this model uses lambda= -0.03432107 and no drift.

Lets just save the model to forecast for year 2020

```{r}
model.arima.best.q3 = Arima(y=as.ts(co2.ts), order=c(0,1,1), seasonal=c(2,0,1), lambda=lambda, include.drift = FALSE);
co2.2020.arima.pred.ts <- forecast(model.arima.best.q3, h=276) %>%
  as_tibble() %>%
  dplyr::select('Point Forecast') %>%
  ts(start = c(1998,1), frequency = 12) %>%
  as_tsibble() %>%
  filter_index("2019-12-31" ~.) 

co2.2020.arima.pred.ts %>%
  autoplot(.vars = value) +
  labs(title="2020 CO2 predictions from best fitted ARIMA (0,1,1) (2,0,1) model")
```



**Part 4 (4 points)**

The file `co2_weekly_mlo.txt` contains weekly observations of atmospheric carbon dioxide concentrations measured at the Mauna Loa Observatory from 1974 to 2020, published by the National Oceanic and Atmospheric Administration (NOAA). Convert these data into a suitable time series object, conduct a thorough EDA on the data, and address the problem of missing observations. Describe how the Keeling Curve evolved from 1997 to the present and compare current atmospheric CO2 levels to those predicted by your forecasts in Parts 2 and 3. Use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2 and 3 over the entire period.  

```{r}
co2.noaa.weekly.df <- read.table('co2_weekly_mlo.txt', header = FALSE, comment.char='#', na.strings = '-999.99')
colnames(co2.noaa.weekly.df) <- c('year','month','day','decimal.day', 'record.value', '#days', 'one.year.ago','ten.years.ago','since.1800.diff')
```

```{r}
head(co2.noaa.weekly.df)
tail(co2.noaa.weekly.df)
```
```{r}
summary(co2.noaa.weekly.df) # 20 readings are missing
describe(co2.noaa.weekly.df)
co2.noaa.weekly.df <- co2.noaa.weekly.df %>%
  mutate(record.date=ymd(paste(year,month,day)))
```


```{r}
week.frequency = 365.25/7
co2.noaa.weekly.ts.raw <- ts(data=co2.noaa.weekly.df$record.value,start = 1974.3795	, frequency = week.frequency)
co2.noaa.weekly.ts <- as_tsibble(ts(data=co2.noaa.weekly.df$record.value,start = 1974.3795	, frequency = week.frequency), class="matrix")
cbind(head(co2.noaa.weekly.ts),tail(co2.noaa.weekly.ts))
summary(co2.noaa.weekly.ts)
```

Missing data analysis. Fill in with interpolation from neighbours. We can try and fit a model and use that model instead for interpolation.
```{r}
co2.noaa.weekly.ts%>%
  filter(is.na(value))

co2.noaa.weekly.ts <- na_interpolation(co2.noaa.weekly.ts, option = "spline")

co2.noaa.weekly.ts%>%
  filter(is.na(value))

```

Plot Keeling curve
```{r}
co2.noaa.weekly.ts %>%
  autoplot(.vars = value) +
  geom_smooth() +
  labs(title = "Keeling Curve till 2020", y="Co2 emission (units)", x="Year")
  
```

We can see that from about 1995 onwards the trend has picked up and increasing faster than pre 1995 trend.

Now lets compare the current CO2 levels to predicted from Q2 and 3

```{r}
co2.noaa.weekly.ts %>%
  filter_index("2019-12-31" ~.) %>%
  mutate(index = as.Date(index)) %>%
  autoplot(.vars = value, color="blue") +
  autolayer(co2.2020.pred.ts, .vars = value, color="red") +
  autolayer(co2.2020.arima.pred.ts, .vars = value, color="navy") +
  labs(title="Year 2020 Actual vs Prediction from Linear Model and ARIMA model")

```
The predicted CO2 levels are lesser compared to actual levels.

Now lets compare monthly average curve with model forecasts
```{r}
#First lets convert to monthly time series
co2.noaa.monthly.ts <- co2.noaa.weekly.ts %>%
  as_tibble() %>%
  mutate(yearmonth=yearmonth(yearweek(index))) %>%
  group_by(yearmonth) %>%
  summarise(avg_value=mean(value)) %>%
  as_tsibble(index = yearmonth, value=avg_value) 

summary(co2.noaa.monthly.ts) #> 1974 May to 2020 Feb


# Now predict using linear model for same time frame
time.index.1974.may = seq(from=(1974-1959)*12+5,length.out=550)
co2.noaa.like.df <- data.frame(time.index= time.index.1974.may, season=factor(time.index.1974.may%%12, ordered = F))
co2.noaa.like.pred <- predict(object = mod.ln.5, newdata = co2.noaa.like.df)
co2.noaa.like.pred.ts <- as_tsibble(ts(data=co2.noaa.like.pred, start = c(1974,5), frequency=12))


# now predict using Arima model selected in Q3
co2.noaa.arima.pred.ts <- forecast(model.arima.best.q3, h=276) %>%
  as_tibble() %>%
  dplyr::select('Point Forecast') %>%
  ts(start = c(1998,1), frequency = 12) %>%
  as_tsibble() 

model.arima.best.q3.fitted.ts <-model.arima.best.q3$fitted %>%
  as_tsibble() 


co2.noaa.monthly.ts %>%
  autoplot(.vars = avg_value, color="blue") +
  autolayer(co2.noaa.like.pred.ts, .vars=value, color="red") +
  autolayer(co2.noaa.arima.pred.ts, .vars=value, color="navy") +
  autolayer(model.arima.best.q3.fitted.ts, .vars=value, color="navy") +
  labs(title="Part 2 and 3 model predictions vs NOAA monthly average")



```

**Part 5 (3 points)**

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.


Lets first see how seasonal decomposition looks
```{r}
co2.noaa.weekly.ts %>%
  stl(s.window = week.frequency) %>%
  autoplot()
```
Now lets substract sesonal data from time series to get sesonally adjusted time series
```{r}
co2.noaa.weekly.ts.components <- stl(co2.noaa.weekly.ts ,s.window = week.frequency)$time.series
co2.noaa.weekly.ts.components.seasonal <- co2.noaa.weekly.ts.components[,'seasonal']
co2.noaa.weekly.ts.SA <- as_tsibble(ts(co2.noaa.weekly.ts$value - co2.noaa.weekly.ts.components.seasonal,start = 1974.3795	, frequency = week.frequency))
cbind(head(co2.noaa.weekly.ts),tail(co2.noaa.weekly.ts),head(co2.noaa.weekly.ts.SA),tail(co2.noaa.weekly.ts.SA))
```

Lets check the SA time series
```{r}
co2.noaa.weekly.ts.SA %>%
  autoplot()

```


```{r}
#Seasonally adjust weekly dataset
co2.noaa.weekly.ts.SA <- stl(co2.noaa.weekly.ts, s.window='periodic') %>% seasadj()
co2.noaa.weekly.ts.SA  %>%  autoplot() + ggtitle("Seasonally Adjusted Weekly NOAA data")

#Split NSA data into training and test
co2.noaa.weekly.ts.SA.test <- tail(co2.noaa.weekly.ts, 104)
co2.noaa.weekly.ts.SA.training <- head(co2.noaa.weekly.ts, 2284)

co2.noaa.weekly.ts.SA.training %>% ggAcf()
co2.noaa.weekly.ts.SA.training %>% ggPacf()
co2.noaa.weekly.ts.SA.training %>% gg_tsdisplay()

#Split NSA data into training and test
co2.noaa.weekly.ts.test <- tail(co2.noaa.weekly.ts, 104)
co2.noaa.weekly.ts.training <- head(co2.noaa.weekly.ts, 2284)

co2.noaa.weekly.ts %>% ggAcf()
co2.noaa.weekly.ts %>% ggPacf()
co2.noaa.weekly.ts %>% gg_tsdisplay()


get.best.arima <- function(x.ts, maxord = c(1,1,1,1,1,1))
{
  best.aic <-1000000
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
      for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in 0:maxord[6])
      {
        fit <- Arima(x.ts, order = c(p,d,q),
                     seas = list(order = c(P,D,Q),
                                 frequency(x.ts)), method = "CSS")
        fit.aic <- -2*fit$loglik + 2*(length(fit$coef))
        if (fit.aic < best.aic)
        {
          best.aic <- fit.aic
          best.fit <- fit
          best.model <- c(p,d,q,P,D,Q)
        }
      }
    list(best.aic, best.fit, best.model)
}

best.arima.training1 <- get.best.arima(as.ts(co2.noaa.weekly.ts.training), maxord = c(1,1,1,1,1,1))
best.arima.training1

best.arima.training2 <- get.best.arima(as.ts(co2.noaa.weekly.ts.SA.training), maxord = c(1,1,1,1,1,1))

best.arima.training1
best.arima.training2

```

```{r gd}
co2.noaa.weekly.ts %>%
  as_tibble() %>%
  as.ts()
ts_seas(co2.noaa.weekly.ts)
```
With 2388 obversations, we can take the last 2 years of the dataset by extracting the last 104 observations, since every observation is 1 week. The remaining 2284 observations are the training set. 

**Part 6 (3 points)**

Generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric C02 levels in the year 2100. How confident are you that these are accurate predictions?










